{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5075b93-fcaa-462a-9330-b8c0d9a36836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c5052-93a2-44be-babf-d9e9e6c4edb8",
   "metadata": {},
   "source": [
    "# Pre-training Protein section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a97bfa-cf59-4ea4-8eed-742ce084b46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GrandBookData/Proteins/tokenizer_data.json']\n"
     ]
    }
   ],
   "source": [
    "#paths = [str(x) for x in Path(\"./GrandBookData/Proteins/\").glob(\"**/*.json\")]\n",
    "paths = ['GrandBookData/Proteins/tokenizer_data.json']\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50078f98-cd24-4ee9-af25-a1e8c1b6371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c55f52-afb2-452a-98cb-50da39d8d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7dfbc56-170f-497d-bcdd-5342e1c855e4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a tokenizer\n",
    "from transformers import LongformerTokenizerFast\n",
    "\n",
    "#tokenizer = LongformerTokenizerFast()\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths,show_progress=True, vocab_size=8192, min_frequency= 10000, special_tokens=[\n",
    "    \"[CLS]\",\n",
    "    \"[PAD]\",\n",
    "    \"[SEP]\",\n",
    "    \"[UNK]\",\n",
    "    \"[MASK]\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffd314b7-3a8e-405a-b224-ce60599c9ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./GrandBookData/ProteinModelDeb/vocab.json',\n",
       " './GrandBookData/ProteinModelDeb/merges.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save files to disk\n",
    "tokenizer.save_model(\"./GrandBookData/ProteinModelDeb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33d99bf1-81f8-4fc9-843a-576d915f0822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=320, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "['[CLS]', 'AL', 'PG', 'HLL', 'RG', 'FLP', 'LL', 'TIF', 'HL', 'ER', 'AE', 'DAA', 'RR', 'AGL', 'ST', 'QPI', 'WR', 'KLW', 'DDV', 'MK', 'TR', 'PPN', 'SE', 'SI', 'TC', 'WR', 'KK', 'FLE', 'TFF', 'SN', 'VL', 'HG', 'VL', 'DV', 'SS', 'DW', 'RL', 'HD', 'RHF', 'SP', 'LLH', 'SSP', 'HV', 'SQL', 'TL', 'CN', 'ML', 'QG', 'AV', 'EL', 'AAE', 'HN', 'HK', 'VLE', 'NL', 'AASL', 'RVL', 'K', 'FQ', 'HLL', 'SC', 'DQ', 'SV', 'RR', 'SL', 'ALLL', 'HRL', 'IH', 'HG', 'SV', 'SQV', 'SM', 'C', 'SW', 'PV', 'PD', 'TV', 'LLVL', 'IL', 'TM', 'SAG', 'F', 'WR', 'SGN', 'AL', 'AY', 'HG', 'SP', 'CGL', 'CR', 'EE', 'DR', 'AQ', 'SQE', 'SAQ', 'EG', 'AQ', 'RG', 'CN', 'AE', 'QEG', 'SGG', 'E', 'KQ', 'KK', 'SP', 'KE', 'AD', 'AR', 'M', 'TS', 'VL', 'SG', 'PR', 'SPPL', 'Q', 'SP', 'AC', 'EE', 'TSS', 'EVP', 'CG', 'HT', 'SIQ', 'GGSS', 'PR', 'SSSE', 'QL', 'SC', 'H', 'PIP', 'RK', 'TRR', 'WQ', 'KP', 'AVG', 'RK', 'RR', 'CL', 'RR', 'SR', 'GH', 'C', 'AD', 'PE', 'DL', 'YD', 'FVF', 'TV', 'ARE', 'DN', 'SG', 'TTE', 'GE', 'TT', 'EN', 'WT', 'SSI', 'PG', 'SP', 'CTG', 'IL', 'SLK', 'AAH', 'RF', 'RSV', 'STL', 'ELF', 'SI', 'PL', 'TG', 'ET', 'CR', 'TL', 'SN', 'LLSS', 'WV', 'SLE', 'NL', 'VL', 'SYN', 'GLN', 'ANI', 'SC', 'IL', 'SGL', 'RAL', 'SRH', 'PE', 'C', 'RF', 'RVL', 'RV', 'SD', 'MF', 'SH', 'MP', 'CM', 'EL', 'VR', 'CIL', 'SAL', 'P', 'QL', 'HTL', 'SV', 'SF', 'DL', 'KN', 'QL', 'EG', 'SR', 'PG', 'ASP', 'SC', 'SE', 'AEI', 'PE', 'SCL', 'E', 'VL', 'EI', 'RFP', 'RE', 'PL', 'HT', 'AFL', 'RP', 'VLK', 'AS', 'KSL', 'QQL', 'SLD', 'SA', 'TLP', 'CP', 'QEL', 'G', 'LLLE', 'VL', 'KE', 'C', 'TS', 'NL', 'KKL', 'SFH', 'DV', 'NL', 'AE', 'HQ', 'KEV', 'LL', 'LLQ', 'DP', 'GL', 'QEI', 'TF', 'SF', 'CRL', 'FE', 'SS', 'TAE', 'FL', 'SE', 'IIN', 'TVK', 'RN', 'SSL', 'KSL', 'KL', 'PGN', 'RL', 'GN', 'H', 'RLV', 'AL', 'ADI', 'F', 'SE', 'DSS', 'SSL', 'CQL', 'DV', 'SSN', 'CI', 'KP', 'DG', 'LLE', 'F', 'TK', 'KL', 'EG', 'HI', 'QQ', 'RGG', 'QLP', 'FT', 'HL', 'RLF', 'QN', 'WL', 'DQ', 'DAE', 'T', 'AQE', 'AL', 'RRL', 'K', 'AVC', 'SVV', 'ND', 'AW', 'DSS', 'H', 'AF', 'AD', 'YI', 'SV', 'M', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./GrandBookData/ProteinModelDeb/vocab.json\",\n",
    "    \"./GrandBookData/ProteinModelDeb/merges.txt\",\n",
    ")\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "print(\n",
    "    tokenizer.encode(\"ALPGHLLRGFLPLLTIFHLERAEDAARRAGLSTQPIWRKLWDDVMKTRPPNSESITCWRKKFLETFFSNVLHGVLDVSSDWRLHDRHFSPLLHSSPHVSQLTLCNMLQGAVELAAEHNHKVLENLAASLRVLKFQHLLSCDQSVRRSLALLLHRLIHHGSVSQVSMCSWPVPDTVLLVLILTMSAGFWRSGNALAYHGSPCGLCREEDRAQSQESAQEGAQRGCNAEQEGSGGEKQKKSPKEADARMTSVLSGPRSPPLQSPACEETSSEVPCGHTSIQGGSSPRSSSEQLSCHPIPRKTRRWQKPAVGRKRRCLRRSRGHCADPEDLYDFVFTVAREDNSGTTEGETTENWTSSIPGSPCTGILSLKAAHRFRSVSTLELFSIPLTGETCRTLSNLLSSWVSLENLVLSYNGLNANISCILSGLRALSRHPECRFRVLRVSDMFSHMPCMELVRCILSALPQLHTLSVSFDLKNQLEGSRPGASPSCSEAEIPESCLEVLEIRFPREPLHTAFLRPVLKASKSLQQLSLDSATLPCPQELGLLLEVLKECTSNLKKLSFHDVNLAEHQKEVLLLLQDPGLQEITFSFCRLFESSTAEFLSEIINTVKRNSSLKSLKLPGNRLGNHRLVALADIFSEDSSSSLCQLDVSSNCIKPDGLLEFTKKLEGHIQQRGGQLPFTHLRLFQNWLDQDAETAQEALRRLKAVCSVVNDAWDSSHAFADYISVM\")\n",
    ")\n",
    "print(tokenizer.encode(\"ALPGHLLRGFLPLLTIFHLERAEDAARRAGLSTQPIWRKLWDDVMKTRPPNSESITCWRKKFLETFFSNVLHGVLDVSSDWRLHDRHFSPLLHSSPHVSQLTLCNMLQGAVELAAEHNHKVLENLAASLRVLKFQHLLSCDQSVRRSLALLLHRLIHHGSVSQVSMCSWPVPDTVLLVLILTMSAGFWRSGNALAYHGSPCGLCREEDRAQSQESAQEGAQRGCNAEQEGSGGEKQKKSPKEADARMTSVLSGPRSPPLQSPACEETSSEVPCGHTSIQGGSSPRSSSEQLSCHPIPRKTRRWQKPAVGRKRRCLRRSRGHCADPEDLYDFVFTVAREDNSGTTEGETTENWTSSIPGSPCTGILSLKAAHRFRSVSTLELFSIPLTGETCRTLSNLLSSWVSLENLVLSYNGLNANISCILSGLRALSRHPECRFRVLRVSDMFSHMPCMELVRCILSALPQLHTLSVSFDLKNQLEGSRPGASPSCSEAEIPESCLEVLEIRFPREPLHTAFLRPVLKASKSLQQLSLDSATLPCPQELGLLLEVLKECTSNLKKLSFHDVNLAEHQKEVLLLLQDPGLQEITFSFCRLFESSTAEFLSEIINTVKRNSSLKSLKLPGNRLGNHRLVALADIFSEDSSSSLCQLDVSSNCIKPDGLLEFTKKLEGHIQQRGGQLPFTHLRLFQNWLDQDAETAQEALRRLKAVCSVVNDAWDSSHAFADYISVM\").tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2322628a-bebe-450d-9c65-f3e1107b6b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'RQ', 'TY', 'TRY', 'QTL', 'ELE', 'KE', 'FH', 'FN', 'RYL', 'TRR', 'RR', 'IEI', 'AH', 'AL', 'CL', 'TE', 'RQI', 'KI', 'WF', 'QN', 'RR', 'MK', 'W', 'KKE', 'NK', 'TKG', 'DIG', 'AN', 'DG', 'SDL', 'SP', 'QT', 'SPQ', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"RQTYTRYQTLELEKEFHFNRYLTRRRRIEIAHALCLTERQIKIWFQNRRMKWKKENKTKGDIGANDGSDLSPQTSPQ\").tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62acfcf8-05e1-4ea7-8ef3-bb3b083ffcf1",
   "metadata": {},
   "source": [
    "# Pre-training Drug section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "015b8054-ba66-47f5-b1b4-41a638a0030f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GrandBookData/Pubchem/tokenizer_data.json']\n"
     ]
    }
   ],
   "source": [
    "#paths = [str(x) for x in Path(\"./GrandBookData/Pubchem/\").glob(\"**/*.json\")]\n",
    "paths = ['GrandBookData/Pubchem/tokenizer_data.json']\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c549668-f5da-4975-9607-3d39c2a0d2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer2 = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer2.train(files=paths,show_progress=True, vocab_size=4096, min_frequency=500, special_tokens=[\n",
    "    \"[CLS]\",\n",
    "    \"[PAD]\",\n",
    "    \"[SEP]\",\n",
    "    \"[UNK]\",\n",
    "    \"[MASK]\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aa501ea9-be92-4807-8efe-9fa226423a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./GrandBookData/PubchemModelDeberta/vocab.json',\n",
       " './GrandBookData/PubchemModelDeberta/merges.txt']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save files to disk\n",
    "tokenizer2.save_model(\"./GrandBookData/PubchemModelDeberta/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "07006426-6108-4a2f-91d6-9b7edc8e81c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./GrandBookData/PubchemModelDeberta/vocab.json\",\n",
    "    \"./GrandBookData/PubchemModelDeberta/merges.txt\",\n",
    ")\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8d17c96-c35e-4e82-9ce8-d56f4bde608f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 272, 12, 39, 274, 51, 13, 39, 12, 51, 13, 39, 263, 51, 13, 51, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"CCC(C)(O)C(O)C(=O)O\").ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e89a1bc4-553e-4d58-a76f-d5d5badd5b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'CCC',\n",
       " '(',\n",
       " 'C',\n",
       " ')(',\n",
       " 'O',\n",
       " ')',\n",
       " 'C',\n",
       " '(',\n",
       " 'O',\n",
       " ')',\n",
       " 'C',\n",
       " '(=',\n",
       " 'O',\n",
       " ')',\n",
       " 'O',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"CCC(C)(O)C(O)C(=O)O\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86201b86-739e-4b7d-a57a-61e7c43a27f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'CC', '[', 'C', '@@]', '1', '(', 'O', ')', 'C', '(=', 'O', ')', 'OCc', '2', 'c', '1', 'cc', '1', 'n', '(', 'c', '2', '=', 'O', ')', 'Cc', '2', 'c', '-', '1', 'nc', '1', 'cc', '(', 'F', ')', 'c', '(', 'C', ')', 'c', '3', 'c', '1', 'c', '2', '[', 'C', '@@', 'H', '](', 'NC', '(=', 'O', ')', 'COCNC', '(=', 'O', ')[', 'C', '@', 'H', '](', 'C', ')', 'NC', '(=', 'O', ')[', 'C', '@@', 'H', '](', 'NC', '(=', 'O', ')[', 'C', '@@', 'H', '](', 'N', ')', 'CCC', '(=', 'O', ')', 'NC', '[', 'C', '@', 'H', ']', '1', 'O', '[', 'C', '@@', 'H', '](', 'CC', '(', 'N', ')=', 'O', ')[', 'C', '@', 'H', '](', 'O', ')[', 'C', '@@', 'H', ']', '1', 'O', ')', 'C', '(', 'C', ')', 'C', ')', 'CC', '3', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2c-1nc1cc(F)c(C)c3c1c2[C@@H](NC(=O)COCNC(=O)[C@H](C)NC(=O)[C@@H](NC(=O)[C@@H](N)CCC(=O)NC[C@H]1O[C@@H](CC(N)=O)[C@H](O)[C@@H]1O)C(C)C)CC3\").tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe232208-f392-4be2-ab8a-62214e4d605c",
   "metadata": {},
   "source": [
    "# Running Drug Pubchem pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb632d1c-5ff5-4dba-853b-3771d1e96ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import DebertaV2Config, DebertaV2Model\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "config = DebertaV2Config(\n",
    "    vocab_size=2692,\n",
    "    max_position_embeddings=512,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    intermediate_size=3072,\n",
    "    hidden_size = 768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baabd7c2-b48e-4932-ad23-5d3178cd2db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DebertaTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DebertaTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaTokenizerFast\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = DebertaTokenizerFast.from_pretrained(\"./GrandBookData/PubchemModelDeberta/\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c10cf4b-091b-4e64-91bd-63c398deaa5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaTokenizerFast(name_or_path='./GrandBookData/PubchemModelDeberta/', vocab_size=2692, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2d834f7-0b4f-42a5-9239-710b87f76434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 272, 12, 39, 274, 51, 13, 39, 12, 51, 13, 39, 263, 51, 13, 51, 2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"CCC(C)(O)C(O)C(=O)O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b1a1d93-07a0-494f-8c78-c12f7098ca9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]CCC(C)(O)C(O)C(=O)O[SEP]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"CCC(C)(O)C(O)C(=O)O\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4368fcbb-1308-4705-aa84-2b07e654cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DebertaV2ForMaskedLM\n",
    "\n",
    "model = DebertaV2ForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a67c194-5eeb-4616-989f-9f7d36679516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45584260"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e26a5f7d-d04a-4e1e-ad4c-5815c31a487c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForMaskedLM(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(2692, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): LegacyDebertaV2OnlyMLMHead(\n",
       "    (predictions): LegacyDebertaV2LMPredictionHead(\n",
       "      (transform): LegacyDebertaV2PredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=2692, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ef54142-2212-40f2-b015-a74eabe2e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b71e0c2-02d9-4c04-ac2a-cabf9fd78d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['GrandBookData/Pubchem/Batch_18.json',\n",
       "  'GrandBookData/Pubchem/Batch_49.json',\n",
       "  'GrandBookData/Pubchem/Batch_58.json',\n",
       "  'GrandBookData/Pubchem/Batch_30.json',\n",
       "  'GrandBookData/Pubchem/Batch_45.json',\n",
       "  'GrandBookData/Pubchem/Batch_23.json',\n",
       "  'GrandBookData/Pubchem/Batch_6.json',\n",
       "  'GrandBookData/Pubchem/Batch_51.json',\n",
       "  'GrandBookData/Pubchem/Batch_50.json',\n",
       "  'GrandBookData/Pubchem/Batch_0.json',\n",
       "  'GrandBookData/Pubchem/Batch_11.json',\n",
       "  'GrandBookData/Pubchem/Batch_34.json',\n",
       "  'GrandBookData/Pubchem/Batch_55.json',\n",
       "  'GrandBookData/Pubchem/Batch_29.json',\n",
       "  'GrandBookData/Pubchem/Batch_12.json',\n",
       "  'GrandBookData/Pubchem/Batch_41.json',\n",
       "  'GrandBookData/Pubchem/Batch_9.json',\n",
       "  'GrandBookData/Pubchem/Batch_47.json',\n",
       "  'GrandBookData/Pubchem/Batch_26.json',\n",
       "  'GrandBookData/Pubchem/Batch_14.json',\n",
       "  'GrandBookData/Pubchem/Batch_15.json',\n",
       "  'GrandBookData/Pubchem/Batch_7.json',\n",
       "  'GrandBookData/Pubchem/Batch_35.json',\n",
       "  'GrandBookData/Pubchem/Batch_46.json',\n",
       "  'GrandBookData/Pubchem/Batch_16.json',\n",
       "  'GrandBookData/Pubchem/Batch_54.json',\n",
       "  'GrandBookData/Pubchem/Batch_28.json',\n",
       "  'GrandBookData/Pubchem/Batch_52.json',\n",
       "  'GrandBookData/Pubchem/Batch_33.json',\n",
       "  'GrandBookData/Pubchem/Batch_20.json',\n",
       "  'GrandBookData/Pubchem/Batch_17.json',\n",
       "  'GrandBookData/Pubchem/Batch_21.json',\n",
       "  'GrandBookData/Pubchem/Batch_5.json',\n",
       "  'GrandBookData/Pubchem/Batch_53.json',\n",
       "  'GrandBookData/Pubchem/Batch_22.json',\n",
       "  'GrandBookData/Pubchem/Batch_57.json',\n",
       "  'GrandBookData/Pubchem/Batch_42.json',\n",
       "  'GrandBookData/Pubchem/Batch_8.json',\n",
       "  'GrandBookData/Pubchem/Batch_19.json',\n",
       "  'GrandBookData/Pubchem/Batch_4.json',\n",
       "  'GrandBookData/Pubchem/Batch_39.json',\n",
       "  'GrandBookData/Pubchem/Batch_1.json',\n",
       "  'GrandBookData/Pubchem/Batch_31.json',\n",
       "  'GrandBookData/Pubchem/Batch_44.json',\n",
       "  'GrandBookData/Pubchem/Batch_43.json',\n",
       "  'GrandBookData/Pubchem/Batch_36.json',\n",
       "  'GrandBookData/Pubchem/Batch_13.json',\n",
       "  'GrandBookData/Pubchem/Batch_48.json',\n",
       "  'GrandBookData/Pubchem/Batch_38.json',\n",
       "  'GrandBookData/Pubchem/Batch_25.json',\n",
       "  'GrandBookData/Pubchem/Batch_27.json',\n",
       "  'GrandBookData/Pubchem/Batch_24.json',\n",
       "  'GrandBookData/Pubchem/Batch_37.json',\n",
       "  'GrandBookData/Pubchem/Batch_32.json',\n",
       "  'GrandBookData/Pubchem/Batch_10.json',\n",
       "  'GrandBookData/Pubchem/Batch_40.json',\n",
       "  'GrandBookData/Pubchem/Batch_3.json',\n",
       "  'GrandBookData/Pubchem/Batch_2.json',\n",
       "  'GrandBookData/Pubchem/Batch_56.json'],\n",
       " 59)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = [str(x) for x in Path(\"./GrandBookData/Pubchem/\").glob(\"**/*.json\")]\n",
    "if 'GrandBookData\\\\Pubchem\\\\tokenizer_data.json' in paths: paths.remove('GrandBookData\\\\Pubchem\\\\tokenizer_data.json')\n",
    "if 'GrandBookData/Pubchem/tokenizer_data.json' in paths: paths.remove('GrandBookData/Pubchem/tokenizer_data.json')\n",
    "paths, len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8efca7a9-57d8-4e70-8004-dd154b81ad0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'CCNCCc1c(C)nc(CSC)nc1C'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files={'train': paths})\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c9bf024-93ca-4d45-99d1-1118434bc8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 117958299\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99b8795b-c509-4ad8-ad2c-e33e7c787b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 117958299\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = dataset['train'].train_test_split(test_size=0.5, seed=42) #comment when running full trainingin\n",
    "dataset = dataset['train']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86bb3e97-4c63-4916-82d8-043b26a20eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117958299"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f145d23b-5549-413d-afd6-d06d6c7ea767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    return tokenizer(example[\"text\"],padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "dataset = dataset.map(tokenization, batched=True, num_proc=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d845d07-5182-4cfa-8a9a-b6194049d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.train_test_split(test_size=0.5, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "296952f1-bd72-4954-a509-0247e4e11047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 58979149\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = dataset['train']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e22d1211-fc89-46ff-b594-ac314099bc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch\n",
      "cuda:0\n",
      "DebertaV2ForMaskedLM(\n",
      "  (deberta): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(2692, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): LegacyDebertaV2OnlyMLMHead(\n",
      "    (predictions): LegacyDebertaV2LMPredictionHead(\n",
      "      (transform): LegacyDebertaV2PredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=2692, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.cuda\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"text\"])\n",
    "print(dataset.format['type'])\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11b3e6d9-c87f-404d-b473-22d80919650f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 58979149\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "898f9038-0f01-4e0c-a030-d7089a1e99af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'O=C(CCS(=O)(=O)c1ccc2c(c1)CCC2)Nc1sccc1C(=O)NC1CC1', 'input_ids': tensor([  0,  51,  33,  39,  12, 353, 263,  51, 287,  51,  13,  71,  21, 264,\n",
      "         22,  71,  12,  71,  21,  13, 272,  22,  13, 284,  21, 421,  21,  39,\n",
      "        263,  51,  13, 270,  21, 262,  21,   2,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50, 512, 512)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_check = dataset[122]\n",
    "print(target_check)\n",
    "len(target_check['text']),len(target_check['input_ids']),len(target_check['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3392bcd-4905-4b96-b9d2-fd3b63348465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O=C(CCS(=O)(=O)c1ccc2c(c1)CCC2)Nc1sccc1C(=O)NC1CC1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS]O=C(CCS(=O)(=O)c1ccc2c(c1)CCC2)Nc1sccc1C(=O)NC1CC1[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(target_check['text'])\n",
    "tokenizer.decode(target_check['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6d910ba-cbfa-416c-9e33-0d5816a8cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variables in the notebook\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "867088e4-9381-4bef-a182-6e8d6ac7a09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['O=c1cc(-c2ccc(F)cc2Cl)nc(C(F)(F)F)[nH]1',\n",
       "  'COc1ccc([C@H](OC(=O)Nc2ccc(C(C)=O)cc2)C(C)(C)CCOC(=O)CS)cc1O',\n",
       "  'C[CH-]OC1C[C@H](C)O[C@@H]1COP(=O)(O)OP(=O)(O)OP(=O)(O)O.[Y]',\n",
       "  'C=CCCN(CCC(=O)OCC)C(=O)OC(C)(C)C',\n",
       "  'CC(C)C(=O)OCCN(CCC(=O)OC(C)(C)C)C(=O)OCc1ccccc1',\n",
       "  'C[C@H](NC(=O)C(O)(c1ccc(-c2ccc3cccnc3n2)cc1)C(F)(F)F)c1ccc(-c2ccc(F)c(C(F)(F)F)c2)cc1',\n",
       "  'CC(=O)N(C1CCN(C2(CC#N)CN(S(C)(=O)=O)C2)CC1)[C@@H]1CC1c1ccccc1',\n",
       "  'O=C(CCc1cc(F)ccc1F)N1CC[C@](O)(c2cn[nH]n2)C1'],\n",
       " 'input_ids': tensor([[  0,  51,  33,  ...,   1,   1,   1],\n",
       "         [  0, 288,  21,  ...,   1,   1,   1],\n",
       "         [  0,  39,  63,  ...,   1,   1,   1],\n",
       "         ...,\n",
       "         [  0,  39,  63,  ...,   1,   1,   1],\n",
       "         [  0, 262, 263,  ...,   1,   1,   1],\n",
       "         [  0,  51,  33,  ...,   1,   1,   1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "batch = dataset[:batch_size]\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af15a1b0-99a1-4692-a177-a17a7fa4e844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 768])\n",
      "tensor([-3.0045,  1.3105, -0.7206, -0.4642, -1.3264,  0.1971, -0.6526, -0.3556,\n",
      "         0.4540, -0.6885, -0.0100, -0.4673,  0.6858, -0.1212,  0.1247, -2.3434,\n",
      "        -1.1263, -1.2363, -0.2208, -0.5745, -1.6735, -1.0859,  0.1919,  1.1737,\n",
      "        -0.1274,  0.2339, -1.0255,  0.0058, -1.1897, -1.4186, -0.1646,  0.9832,\n",
      "         1.4349, -1.1743,  0.1341, -0.8001, -0.3570, -0.2792, -0.4497, -1.3760,\n",
      "        -0.6037, -1.0886, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129,\n",
      "        -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129, -1.3129],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6762/4054158892.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(batch['input_ids']).to(device)\n",
      "/tmp/ipykernel_6762/4054158892.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(batch['attention_mask']).to(device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extract the input features (input_ids, token_type_ids, attention_mask)\n",
    "input_ids = torch.tensor(batch['input_ids']).to(device)\n",
    "attention_mask = torch.tensor(batch['attention_mask']).to(device)\n",
    "\n",
    "# Make sure to set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Pass the batch through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model.deberta(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "# Extract the last hidden state\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "# Print the shape of the last hidden state (should be: [batch_size, seq_length, hidden_size])\n",
    "print(last_hidden_state[1].shape)  # Expected shape: (8, seq_length, 768)\n",
    "\n",
    "# Optionally, print the actual dummy values of the last hidden state (for illustration)\n",
    "print(last_hidden_state[1,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e459d1d0-926d-4708-bb77-b2716c3d0076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam8bit (\n",
       "Parameter Group 0\n",
       "    alpha: 0.0\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0001\n",
       "    t_alpha: None\n",
       "    t_beta3: None\n",
       "    weight_decay: 0.001\n",
       "\n",
       "Parameter Group 1\n",
       "    alpha: 0.0\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0001\n",
       "    t_alpha: None\n",
       "    t_beta3: None\n",
       "    weight_decay: 0.0\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import bitsandbytes as bnb\n",
    "from torch import nn\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "def get_optimizer(model,n):\n",
    "    training_args = TrainingArguments(\n",
    "        #torch_compile=True,\n",
    "        output_dir=\"./GrandBookData/OutPubchemDeberta\",\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay = 1e-3,\n",
    "        do_train=True,\n",
    "        num_train_epochs=3,\n",
    "        fp16=True,\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=2,\n",
    "        #NCCL_P2P_DISABLE=\"1\", \n",
    "        #NCCL_IB_DISABLE=\"1\",\n",
    "        #gradient_checkpointing=True,\n",
    "        save_steps=10_000,\n",
    "    save_total_limit=5,\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "    #model = multitask_model.taskmodels_dict[\"tox21\"]\n",
    "    decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "            \"weight_decay\": training_args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    optimizer_kwargs = {\n",
    "        \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n",
    "        \"eps\": training_args.adam_epsilon,\n",
    "    }\n",
    "    optimizer_kwargs[\"lr\"] = training_args.learning_rate\n",
    "    adam_bnb_optim = bnb.optim.Adam8bit(\n",
    "        optimizer_grouped_parameters,\n",
    "        betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
    "        eps=training_args.adam_epsilon,\n",
    "        lr=training_args.learning_rate,\n",
    "    )\n",
    "    return adam_bnb_optim, training_args\n",
    "\n",
    "optimizer,training_args = get_optimizer(model,\"e\")\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40a25338-4daa-4080-ba09-f9a7c2c0d331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForMaskedLM(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(2692, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): LegacyDebertaV2OnlyMLMHead(\n",
       "    (predictions): LegacyDebertaV2LMPredictionHead(\n",
       "      (transform): LegacyDebertaV2PredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=2692, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4f53abf-e3bc-4c6f-a7b0-367fb7efbda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    optimizers=(optimizer, None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34e0adb2-2d05-4d5c-b282-81cb35318ebb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvlad-mun\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vlad/FebReserach/Work ML/Paracelsus_Main/wandb/run-20250227_163603-uc9zzddt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vlad-mun/huggingface/runs/uc9zzddt' target=\"_blank\">./GrandBookData/OutPubchemDeberta</a></strong> to <a href='https://wandb.ai/vlad-mun/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vlad-mun/huggingface' target=\"_blank\">https://wandb.ai/vlad-mun/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vlad-mun/huggingface/runs/uc9zzddt' target=\"_blank\">https://wandb.ai/vlad-mun/huggingface/runs/uc9zzddt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460776' max='460776' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460776/460776 83:51:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.084600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.087800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.877500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.755300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.622200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.581100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.526600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.503400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.487300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.472800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.459900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.449300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.439000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.430300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.425100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.404900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.398100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.390400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.386300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.377400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.372000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.367800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.361600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.358400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.359600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.355400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.353900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.350800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.347600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.347100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.341400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.341900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.336300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.334200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.334400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.333500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.330300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.331800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.330400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.326300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.327400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.328200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.326000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.324100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.322100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.321100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.319800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.319600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.319600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.318600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.315900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.316400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.314700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.310400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.312200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.312400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.309600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.310900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.307900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.310100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.308400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.306900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.304900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.305600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.305100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.304500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.303100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.303800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.301600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.299100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.297600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.296700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.297400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.296400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>0.296100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.293600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.296300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>0.292300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.292800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>0.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.291600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.291800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.288900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>0.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.289300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>0.290200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.290500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>0.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>0.289200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.288900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>0.286600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.286300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>0.285500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>0.287600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.284500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>0.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.284900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>0.285400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.285100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>0.284500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.284600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>0.283800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>0.281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.282200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>0.280500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.282200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>0.282700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>0.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.279400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>0.280500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>0.281100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>0.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>0.278100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>0.279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.277400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>0.277500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.277100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>0.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>0.275500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79500</td>\n",
       "      <td>0.276600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80500</td>\n",
       "      <td>0.276600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>0.274700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81500</td>\n",
       "      <td>0.273700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>0.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>0.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83500</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.275100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84500</td>\n",
       "      <td>0.273600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.275400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85500</td>\n",
       "      <td>0.272700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.272100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86500</td>\n",
       "      <td>0.273100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>0.274900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87500</td>\n",
       "      <td>0.272100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.270400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88500</td>\n",
       "      <td>0.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>0.269800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89500</td>\n",
       "      <td>0.270300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.271200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90500</td>\n",
       "      <td>0.271800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>0.269600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91500</td>\n",
       "      <td>0.269700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.269800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92500</td>\n",
       "      <td>0.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>0.269900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93500</td>\n",
       "      <td>0.268600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>0.269100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94500</td>\n",
       "      <td>0.268900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95500</td>\n",
       "      <td>0.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.268600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96500</td>\n",
       "      <td>0.269100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>0.269800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97500</td>\n",
       "      <td>0.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>0.269300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98500</td>\n",
       "      <td>0.266400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>0.266400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99500</td>\n",
       "      <td>0.266200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100500</td>\n",
       "      <td>0.268700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>0.266800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101500</td>\n",
       "      <td>0.265800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.265900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102500</td>\n",
       "      <td>0.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103000</td>\n",
       "      <td>0.266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103500</td>\n",
       "      <td>0.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.265900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104500</td>\n",
       "      <td>0.264700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>0.265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105500</td>\n",
       "      <td>0.264700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>0.263500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106500</td>\n",
       "      <td>0.263100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107000</td>\n",
       "      <td>0.264100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107500</td>\n",
       "      <td>0.264600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.264600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108500</td>\n",
       "      <td>0.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109000</td>\n",
       "      <td>0.262900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109500</td>\n",
       "      <td>0.262700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110500</td>\n",
       "      <td>0.263500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111000</td>\n",
       "      <td>0.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111500</td>\n",
       "      <td>0.262200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112500</td>\n",
       "      <td>0.263300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113000</td>\n",
       "      <td>0.261400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113500</td>\n",
       "      <td>0.262200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.262400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114500</td>\n",
       "      <td>0.261800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>0.261400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115500</td>\n",
       "      <td>0.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116500</td>\n",
       "      <td>0.262400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117000</td>\n",
       "      <td>0.260300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117500</td>\n",
       "      <td>0.261300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118500</td>\n",
       "      <td>0.259800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119000</td>\n",
       "      <td>0.258300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119500</td>\n",
       "      <td>0.260200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.258900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120500</td>\n",
       "      <td>0.260500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121000</td>\n",
       "      <td>0.261300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121500</td>\n",
       "      <td>0.258700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.257700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122500</td>\n",
       "      <td>0.258600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123000</td>\n",
       "      <td>0.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123500</td>\n",
       "      <td>0.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124500</td>\n",
       "      <td>0.258800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>0.258500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125500</td>\n",
       "      <td>0.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126500</td>\n",
       "      <td>0.256400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127000</td>\n",
       "      <td>0.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127500</td>\n",
       "      <td>0.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128500</td>\n",
       "      <td>0.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129000</td>\n",
       "      <td>0.257200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129500</td>\n",
       "      <td>0.256700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130500</td>\n",
       "      <td>0.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131000</td>\n",
       "      <td>0.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131500</td>\n",
       "      <td>0.255200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>0.256300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132500</td>\n",
       "      <td>0.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133000</td>\n",
       "      <td>0.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133500</td>\n",
       "      <td>0.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>0.253800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134500</td>\n",
       "      <td>0.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>0.254500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135500</td>\n",
       "      <td>0.254700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>0.254900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136500</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137000</td>\n",
       "      <td>0.254900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137500</td>\n",
       "      <td>0.253900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>0.255700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138500</td>\n",
       "      <td>0.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139000</td>\n",
       "      <td>0.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139500</td>\n",
       "      <td>0.254500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140500</td>\n",
       "      <td>0.253200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141000</td>\n",
       "      <td>0.253600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141500</td>\n",
       "      <td>0.254400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142500</td>\n",
       "      <td>0.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143000</td>\n",
       "      <td>0.251300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143500</td>\n",
       "      <td>0.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>0.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144500</td>\n",
       "      <td>0.254700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>0.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145500</td>\n",
       "      <td>0.252400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146000</td>\n",
       "      <td>0.251700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146500</td>\n",
       "      <td>0.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147000</td>\n",
       "      <td>0.251500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147500</td>\n",
       "      <td>0.252900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148000</td>\n",
       "      <td>0.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148500</td>\n",
       "      <td>0.251700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149000</td>\n",
       "      <td>0.251100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149500</td>\n",
       "      <td>0.253500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>0.251300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150500</td>\n",
       "      <td>0.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151000</td>\n",
       "      <td>0.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151500</td>\n",
       "      <td>0.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152000</td>\n",
       "      <td>0.249400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152500</td>\n",
       "      <td>0.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153000</td>\n",
       "      <td>0.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153500</td>\n",
       "      <td>0.249700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154000</td>\n",
       "      <td>0.251800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154500</td>\n",
       "      <td>0.249400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155000</td>\n",
       "      <td>0.250100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155500</td>\n",
       "      <td>0.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156000</td>\n",
       "      <td>0.251100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156500</td>\n",
       "      <td>0.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157000</td>\n",
       "      <td>0.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157500</td>\n",
       "      <td>0.248800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158000</td>\n",
       "      <td>0.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158500</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159000</td>\n",
       "      <td>0.248600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159500</td>\n",
       "      <td>0.249300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>0.248800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160500</td>\n",
       "      <td>0.247700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161000</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161500</td>\n",
       "      <td>0.248300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162000</td>\n",
       "      <td>0.248300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162500</td>\n",
       "      <td>0.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163000</td>\n",
       "      <td>0.247500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163500</td>\n",
       "      <td>0.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164000</td>\n",
       "      <td>0.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164500</td>\n",
       "      <td>0.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165000</td>\n",
       "      <td>0.245200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165500</td>\n",
       "      <td>0.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166000</td>\n",
       "      <td>0.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166500</td>\n",
       "      <td>0.246900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167000</td>\n",
       "      <td>0.246400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167500</td>\n",
       "      <td>0.248600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168000</td>\n",
       "      <td>0.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168500</td>\n",
       "      <td>0.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169000</td>\n",
       "      <td>0.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169500</td>\n",
       "      <td>0.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>0.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170500</td>\n",
       "      <td>0.246300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171000</td>\n",
       "      <td>0.246400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171500</td>\n",
       "      <td>0.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172000</td>\n",
       "      <td>0.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172500</td>\n",
       "      <td>0.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173000</td>\n",
       "      <td>0.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173500</td>\n",
       "      <td>0.243800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174000</td>\n",
       "      <td>0.246800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174500</td>\n",
       "      <td>0.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175000</td>\n",
       "      <td>0.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175500</td>\n",
       "      <td>0.245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176000</td>\n",
       "      <td>0.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176500</td>\n",
       "      <td>0.245800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177000</td>\n",
       "      <td>0.243700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177500</td>\n",
       "      <td>0.244700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178000</td>\n",
       "      <td>0.243200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178500</td>\n",
       "      <td>0.244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179000</td>\n",
       "      <td>0.243200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179500</td>\n",
       "      <td>0.244900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>0.243500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180500</td>\n",
       "      <td>0.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181000</td>\n",
       "      <td>0.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181500</td>\n",
       "      <td>0.243300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182000</td>\n",
       "      <td>0.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182500</td>\n",
       "      <td>0.241700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183000</td>\n",
       "      <td>0.243100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183500</td>\n",
       "      <td>0.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184000</td>\n",
       "      <td>0.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184500</td>\n",
       "      <td>0.241300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185000</td>\n",
       "      <td>0.242900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185500</td>\n",
       "      <td>0.243500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186000</td>\n",
       "      <td>0.243200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186500</td>\n",
       "      <td>0.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187000</td>\n",
       "      <td>0.241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187500</td>\n",
       "      <td>0.242200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188000</td>\n",
       "      <td>0.242400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188500</td>\n",
       "      <td>0.241600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189000</td>\n",
       "      <td>0.242900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189500</td>\n",
       "      <td>0.243100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190000</td>\n",
       "      <td>0.243300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190500</td>\n",
       "      <td>0.241900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191000</td>\n",
       "      <td>0.243200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191500</td>\n",
       "      <td>0.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192000</td>\n",
       "      <td>0.241800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192500</td>\n",
       "      <td>0.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193000</td>\n",
       "      <td>0.241400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193500</td>\n",
       "      <td>0.241500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194000</td>\n",
       "      <td>0.241700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194500</td>\n",
       "      <td>0.242100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195000</td>\n",
       "      <td>0.241200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195500</td>\n",
       "      <td>0.241600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196000</td>\n",
       "      <td>0.240300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196500</td>\n",
       "      <td>0.243700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197000</td>\n",
       "      <td>0.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197500</td>\n",
       "      <td>0.240700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198000</td>\n",
       "      <td>0.241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198500</td>\n",
       "      <td>0.240200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199000</td>\n",
       "      <td>0.241700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199500</td>\n",
       "      <td>0.240300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200000</td>\n",
       "      <td>0.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200500</td>\n",
       "      <td>0.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201000</td>\n",
       "      <td>0.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201500</td>\n",
       "      <td>0.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202000</td>\n",
       "      <td>0.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202500</td>\n",
       "      <td>0.239500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203000</td>\n",
       "      <td>0.239500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203500</td>\n",
       "      <td>0.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204000</td>\n",
       "      <td>0.239800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204500</td>\n",
       "      <td>0.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205000</td>\n",
       "      <td>0.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205500</td>\n",
       "      <td>0.240400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206000</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206500</td>\n",
       "      <td>0.239600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207000</td>\n",
       "      <td>0.240100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207500</td>\n",
       "      <td>0.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208000</td>\n",
       "      <td>0.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208500</td>\n",
       "      <td>0.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209000</td>\n",
       "      <td>0.238600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209500</td>\n",
       "      <td>0.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210000</td>\n",
       "      <td>0.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210500</td>\n",
       "      <td>0.238600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211000</td>\n",
       "      <td>0.237700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211500</td>\n",
       "      <td>0.239600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212000</td>\n",
       "      <td>0.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212500</td>\n",
       "      <td>0.238500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213000</td>\n",
       "      <td>0.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213500</td>\n",
       "      <td>0.238500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214000</td>\n",
       "      <td>0.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214500</td>\n",
       "      <td>0.237700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215000</td>\n",
       "      <td>0.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215500</td>\n",
       "      <td>0.238600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216000</td>\n",
       "      <td>0.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216500</td>\n",
       "      <td>0.236300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217000</td>\n",
       "      <td>0.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217500</td>\n",
       "      <td>0.238100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218000</td>\n",
       "      <td>0.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218500</td>\n",
       "      <td>0.237700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219000</td>\n",
       "      <td>0.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219500</td>\n",
       "      <td>0.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220000</td>\n",
       "      <td>0.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220500</td>\n",
       "      <td>0.235600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221000</td>\n",
       "      <td>0.237400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221500</td>\n",
       "      <td>0.236800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222000</td>\n",
       "      <td>0.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222500</td>\n",
       "      <td>0.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223000</td>\n",
       "      <td>0.235600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223500</td>\n",
       "      <td>0.236300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224000</td>\n",
       "      <td>0.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224500</td>\n",
       "      <td>0.235900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225000</td>\n",
       "      <td>0.236700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225500</td>\n",
       "      <td>0.237100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226000</td>\n",
       "      <td>0.235800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226500</td>\n",
       "      <td>0.233900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227000</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227500</td>\n",
       "      <td>0.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228000</td>\n",
       "      <td>0.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228500</td>\n",
       "      <td>0.235600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229000</td>\n",
       "      <td>0.235800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229500</td>\n",
       "      <td>0.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230000</td>\n",
       "      <td>0.235500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230500</td>\n",
       "      <td>0.235400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231000</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231500</td>\n",
       "      <td>0.234300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232000</td>\n",
       "      <td>0.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232500</td>\n",
       "      <td>0.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233000</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233500</td>\n",
       "      <td>0.234300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234000</td>\n",
       "      <td>0.233900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234500</td>\n",
       "      <td>0.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235000</td>\n",
       "      <td>0.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235500</td>\n",
       "      <td>0.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236000</td>\n",
       "      <td>0.233100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236500</td>\n",
       "      <td>0.232700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237000</td>\n",
       "      <td>0.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237500</td>\n",
       "      <td>0.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238000</td>\n",
       "      <td>0.234100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238500</td>\n",
       "      <td>0.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239000</td>\n",
       "      <td>0.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239500</td>\n",
       "      <td>0.232700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240000</td>\n",
       "      <td>0.233200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240500</td>\n",
       "      <td>0.233700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241000</td>\n",
       "      <td>0.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241500</td>\n",
       "      <td>0.233200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242000</td>\n",
       "      <td>0.231700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242500</td>\n",
       "      <td>0.233900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243000</td>\n",
       "      <td>0.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243500</td>\n",
       "      <td>0.234700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244000</td>\n",
       "      <td>0.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244500</td>\n",
       "      <td>0.232600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245000</td>\n",
       "      <td>0.232900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245500</td>\n",
       "      <td>0.233900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246000</td>\n",
       "      <td>0.231900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246500</td>\n",
       "      <td>0.231700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247000</td>\n",
       "      <td>0.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247500</td>\n",
       "      <td>0.233200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248000</td>\n",
       "      <td>0.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248500</td>\n",
       "      <td>0.231900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249000</td>\n",
       "      <td>0.230800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249500</td>\n",
       "      <td>0.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250000</td>\n",
       "      <td>0.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250500</td>\n",
       "      <td>0.231500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251000</td>\n",
       "      <td>0.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251500</td>\n",
       "      <td>0.230700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252000</td>\n",
       "      <td>0.232700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252500</td>\n",
       "      <td>0.231900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253000</td>\n",
       "      <td>0.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253500</td>\n",
       "      <td>0.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254000</td>\n",
       "      <td>0.232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254500</td>\n",
       "      <td>0.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255000</td>\n",
       "      <td>0.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255500</td>\n",
       "      <td>0.229700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256000</td>\n",
       "      <td>0.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256500</td>\n",
       "      <td>0.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257000</td>\n",
       "      <td>0.230700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257500</td>\n",
       "      <td>0.229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258000</td>\n",
       "      <td>0.232600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258500</td>\n",
       "      <td>0.228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259000</td>\n",
       "      <td>0.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259500</td>\n",
       "      <td>0.230800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260000</td>\n",
       "      <td>0.232600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260500</td>\n",
       "      <td>0.229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261000</td>\n",
       "      <td>0.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261500</td>\n",
       "      <td>0.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262000</td>\n",
       "      <td>0.228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262500</td>\n",
       "      <td>0.231400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263000</td>\n",
       "      <td>0.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263500</td>\n",
       "      <td>0.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264000</td>\n",
       "      <td>0.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264500</td>\n",
       "      <td>0.229900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265000</td>\n",
       "      <td>0.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265500</td>\n",
       "      <td>0.232100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266000</td>\n",
       "      <td>0.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266500</td>\n",
       "      <td>0.229600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267000</td>\n",
       "      <td>0.230200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267500</td>\n",
       "      <td>0.230200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268000</td>\n",
       "      <td>0.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268500</td>\n",
       "      <td>0.229200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269000</td>\n",
       "      <td>0.230500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269500</td>\n",
       "      <td>0.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270000</td>\n",
       "      <td>0.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270500</td>\n",
       "      <td>0.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271000</td>\n",
       "      <td>0.229500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271500</td>\n",
       "      <td>0.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272000</td>\n",
       "      <td>0.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272500</td>\n",
       "      <td>0.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273000</td>\n",
       "      <td>0.227600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273500</td>\n",
       "      <td>0.229200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274000</td>\n",
       "      <td>0.228200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274500</td>\n",
       "      <td>0.229100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275000</td>\n",
       "      <td>0.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275500</td>\n",
       "      <td>0.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276000</td>\n",
       "      <td>0.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276500</td>\n",
       "      <td>0.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277000</td>\n",
       "      <td>0.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277500</td>\n",
       "      <td>0.226900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278000</td>\n",
       "      <td>0.227600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278500</td>\n",
       "      <td>0.227200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279000</td>\n",
       "      <td>0.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279500</td>\n",
       "      <td>0.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280000</td>\n",
       "      <td>0.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280500</td>\n",
       "      <td>0.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281000</td>\n",
       "      <td>0.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281500</td>\n",
       "      <td>0.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282000</td>\n",
       "      <td>0.227400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282500</td>\n",
       "      <td>0.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283000</td>\n",
       "      <td>0.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283500</td>\n",
       "      <td>0.225700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284000</td>\n",
       "      <td>0.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284500</td>\n",
       "      <td>0.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285000</td>\n",
       "      <td>0.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285500</td>\n",
       "      <td>0.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286000</td>\n",
       "      <td>0.228100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286500</td>\n",
       "      <td>0.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287000</td>\n",
       "      <td>0.228200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287500</td>\n",
       "      <td>0.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288000</td>\n",
       "      <td>0.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288500</td>\n",
       "      <td>0.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289000</td>\n",
       "      <td>0.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289500</td>\n",
       "      <td>0.225800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290000</td>\n",
       "      <td>0.228100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290500</td>\n",
       "      <td>0.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291000</td>\n",
       "      <td>0.223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291500</td>\n",
       "      <td>0.227200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292000</td>\n",
       "      <td>0.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292500</td>\n",
       "      <td>0.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293000</td>\n",
       "      <td>0.225900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293500</td>\n",
       "      <td>0.227200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294000</td>\n",
       "      <td>0.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294500</td>\n",
       "      <td>0.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295000</td>\n",
       "      <td>0.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295500</td>\n",
       "      <td>0.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296000</td>\n",
       "      <td>0.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296500</td>\n",
       "      <td>0.226100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297000</td>\n",
       "      <td>0.226700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297500</td>\n",
       "      <td>0.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298000</td>\n",
       "      <td>0.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298500</td>\n",
       "      <td>0.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299000</td>\n",
       "      <td>0.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299500</td>\n",
       "      <td>0.223100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300000</td>\n",
       "      <td>0.223900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300500</td>\n",
       "      <td>0.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301000</td>\n",
       "      <td>0.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301500</td>\n",
       "      <td>0.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302000</td>\n",
       "      <td>0.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302500</td>\n",
       "      <td>0.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303000</td>\n",
       "      <td>0.224900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303500</td>\n",
       "      <td>0.225700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304000</td>\n",
       "      <td>0.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304500</td>\n",
       "      <td>0.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305000</td>\n",
       "      <td>0.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305500</td>\n",
       "      <td>0.222700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306000</td>\n",
       "      <td>0.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306500</td>\n",
       "      <td>0.224900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307000</td>\n",
       "      <td>0.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307500</td>\n",
       "      <td>0.223900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308000</td>\n",
       "      <td>0.223900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308500</td>\n",
       "      <td>0.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309000</td>\n",
       "      <td>0.223100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309500</td>\n",
       "      <td>0.223200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310000</td>\n",
       "      <td>0.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310500</td>\n",
       "      <td>0.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311000</td>\n",
       "      <td>0.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311500</td>\n",
       "      <td>0.222800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312000</td>\n",
       "      <td>0.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312500</td>\n",
       "      <td>0.223100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313000</td>\n",
       "      <td>0.223900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313500</td>\n",
       "      <td>0.222600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314000</td>\n",
       "      <td>0.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314500</td>\n",
       "      <td>0.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315000</td>\n",
       "      <td>0.221700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315500</td>\n",
       "      <td>0.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316000</td>\n",
       "      <td>0.221500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316500</td>\n",
       "      <td>0.224300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317000</td>\n",
       "      <td>0.222800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317500</td>\n",
       "      <td>0.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318000</td>\n",
       "      <td>0.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318500</td>\n",
       "      <td>0.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319000</td>\n",
       "      <td>0.223300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319500</td>\n",
       "      <td>0.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320000</td>\n",
       "      <td>0.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320500</td>\n",
       "      <td>0.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321000</td>\n",
       "      <td>0.222500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321500</td>\n",
       "      <td>0.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322000</td>\n",
       "      <td>0.223200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322500</td>\n",
       "      <td>0.220900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323000</td>\n",
       "      <td>0.220700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323500</td>\n",
       "      <td>0.220100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324000</td>\n",
       "      <td>0.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324500</td>\n",
       "      <td>0.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325000</td>\n",
       "      <td>0.221400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325500</td>\n",
       "      <td>0.219800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326000</td>\n",
       "      <td>0.221400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326500</td>\n",
       "      <td>0.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327000</td>\n",
       "      <td>0.222900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327500</td>\n",
       "      <td>0.221400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328000</td>\n",
       "      <td>0.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328500</td>\n",
       "      <td>0.222800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329000</td>\n",
       "      <td>0.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329500</td>\n",
       "      <td>0.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330000</td>\n",
       "      <td>0.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330500</td>\n",
       "      <td>0.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331000</td>\n",
       "      <td>0.221500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331500</td>\n",
       "      <td>0.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332000</td>\n",
       "      <td>0.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332500</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333000</td>\n",
       "      <td>0.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333500</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334000</td>\n",
       "      <td>0.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334500</td>\n",
       "      <td>0.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335000</td>\n",
       "      <td>0.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335500</td>\n",
       "      <td>0.220100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336000</td>\n",
       "      <td>0.220500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336500</td>\n",
       "      <td>0.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337000</td>\n",
       "      <td>0.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337500</td>\n",
       "      <td>0.219800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338000</td>\n",
       "      <td>0.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338500</td>\n",
       "      <td>0.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339000</td>\n",
       "      <td>0.219400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339500</td>\n",
       "      <td>0.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340000</td>\n",
       "      <td>0.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340500</td>\n",
       "      <td>0.220600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341000</td>\n",
       "      <td>0.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341500</td>\n",
       "      <td>0.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342000</td>\n",
       "      <td>0.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342500</td>\n",
       "      <td>0.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343000</td>\n",
       "      <td>0.220900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343500</td>\n",
       "      <td>0.220800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344000</td>\n",
       "      <td>0.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344500</td>\n",
       "      <td>0.219900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345000</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345500</td>\n",
       "      <td>0.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346000</td>\n",
       "      <td>0.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346500</td>\n",
       "      <td>0.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347000</td>\n",
       "      <td>0.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347500</td>\n",
       "      <td>0.220700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348000</td>\n",
       "      <td>0.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348500</td>\n",
       "      <td>0.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349000</td>\n",
       "      <td>0.220600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349500</td>\n",
       "      <td>0.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350000</td>\n",
       "      <td>0.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350500</td>\n",
       "      <td>0.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351000</td>\n",
       "      <td>0.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351500</td>\n",
       "      <td>0.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352000</td>\n",
       "      <td>0.216900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352500</td>\n",
       "      <td>0.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353000</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353500</td>\n",
       "      <td>0.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354000</td>\n",
       "      <td>0.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354500</td>\n",
       "      <td>0.217100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355000</td>\n",
       "      <td>0.216200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355500</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356000</td>\n",
       "      <td>0.218400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356500</td>\n",
       "      <td>0.218400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357000</td>\n",
       "      <td>0.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357500</td>\n",
       "      <td>0.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358000</td>\n",
       "      <td>0.218200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358500</td>\n",
       "      <td>0.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359000</td>\n",
       "      <td>0.216100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359500</td>\n",
       "      <td>0.217100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360000</td>\n",
       "      <td>0.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360500</td>\n",
       "      <td>0.217500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361000</td>\n",
       "      <td>0.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361500</td>\n",
       "      <td>0.216700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362000</td>\n",
       "      <td>0.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362500</td>\n",
       "      <td>0.217500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363000</td>\n",
       "      <td>0.217100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363500</td>\n",
       "      <td>0.218100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364000</td>\n",
       "      <td>0.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364500</td>\n",
       "      <td>0.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365000</td>\n",
       "      <td>0.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365500</td>\n",
       "      <td>0.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366000</td>\n",
       "      <td>0.217500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366500</td>\n",
       "      <td>0.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367000</td>\n",
       "      <td>0.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367500</td>\n",
       "      <td>0.217300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368000</td>\n",
       "      <td>0.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368500</td>\n",
       "      <td>0.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369000</td>\n",
       "      <td>0.217100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369500</td>\n",
       "      <td>0.217200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370000</td>\n",
       "      <td>0.217700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370500</td>\n",
       "      <td>0.216100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371000</td>\n",
       "      <td>0.216200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371500</td>\n",
       "      <td>0.215900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372000</td>\n",
       "      <td>0.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372500</td>\n",
       "      <td>0.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373000</td>\n",
       "      <td>0.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373500</td>\n",
       "      <td>0.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374000</td>\n",
       "      <td>0.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374500</td>\n",
       "      <td>0.216100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375000</td>\n",
       "      <td>0.217300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375500</td>\n",
       "      <td>0.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376000</td>\n",
       "      <td>0.215800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376500</td>\n",
       "      <td>0.214900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377000</td>\n",
       "      <td>0.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377500</td>\n",
       "      <td>0.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378000</td>\n",
       "      <td>0.216100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378500</td>\n",
       "      <td>0.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379000</td>\n",
       "      <td>0.216800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379500</td>\n",
       "      <td>0.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380000</td>\n",
       "      <td>0.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380500</td>\n",
       "      <td>0.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381000</td>\n",
       "      <td>0.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381500</td>\n",
       "      <td>0.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382000</td>\n",
       "      <td>0.215300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382500</td>\n",
       "      <td>0.217200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>383000</td>\n",
       "      <td>0.214200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>383500</td>\n",
       "      <td>0.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384000</td>\n",
       "      <td>0.216800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384500</td>\n",
       "      <td>0.216100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385000</td>\n",
       "      <td>0.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385500</td>\n",
       "      <td>0.214600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>386000</td>\n",
       "      <td>0.214300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>386500</td>\n",
       "      <td>0.215900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387000</td>\n",
       "      <td>0.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387500</td>\n",
       "      <td>0.213800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388000</td>\n",
       "      <td>0.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388500</td>\n",
       "      <td>0.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389000</td>\n",
       "      <td>0.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389500</td>\n",
       "      <td>0.216200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390000</td>\n",
       "      <td>0.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390500</td>\n",
       "      <td>0.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>391000</td>\n",
       "      <td>0.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>391500</td>\n",
       "      <td>0.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392000</td>\n",
       "      <td>0.215300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392500</td>\n",
       "      <td>0.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393000</td>\n",
       "      <td>0.214200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393500</td>\n",
       "      <td>0.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394000</td>\n",
       "      <td>0.214900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394500</td>\n",
       "      <td>0.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395000</td>\n",
       "      <td>0.213900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395500</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396000</td>\n",
       "      <td>0.213600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396500</td>\n",
       "      <td>0.214200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397000</td>\n",
       "      <td>0.213600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397500</td>\n",
       "      <td>0.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398000</td>\n",
       "      <td>0.213800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398500</td>\n",
       "      <td>0.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399000</td>\n",
       "      <td>0.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399500</td>\n",
       "      <td>0.214300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400000</td>\n",
       "      <td>0.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400500</td>\n",
       "      <td>0.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401000</td>\n",
       "      <td>0.212600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401500</td>\n",
       "      <td>0.214300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402000</td>\n",
       "      <td>0.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402500</td>\n",
       "      <td>0.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403000</td>\n",
       "      <td>0.214100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403500</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404000</td>\n",
       "      <td>0.212900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404500</td>\n",
       "      <td>0.214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405000</td>\n",
       "      <td>0.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405500</td>\n",
       "      <td>0.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406000</td>\n",
       "      <td>0.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406500</td>\n",
       "      <td>0.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407000</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407500</td>\n",
       "      <td>0.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408000</td>\n",
       "      <td>0.212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408500</td>\n",
       "      <td>0.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409000</td>\n",
       "      <td>0.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409500</td>\n",
       "      <td>0.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410000</td>\n",
       "      <td>0.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410500</td>\n",
       "      <td>0.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411000</td>\n",
       "      <td>0.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411500</td>\n",
       "      <td>0.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412000</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412500</td>\n",
       "      <td>0.214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413000</td>\n",
       "      <td>0.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413500</td>\n",
       "      <td>0.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414000</td>\n",
       "      <td>0.212900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414500</td>\n",
       "      <td>0.213900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415000</td>\n",
       "      <td>0.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415500</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416000</td>\n",
       "      <td>0.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416500</td>\n",
       "      <td>0.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417000</td>\n",
       "      <td>0.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417500</td>\n",
       "      <td>0.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418000</td>\n",
       "      <td>0.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418500</td>\n",
       "      <td>0.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419000</td>\n",
       "      <td>0.212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419500</td>\n",
       "      <td>0.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420000</td>\n",
       "      <td>0.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420500</td>\n",
       "      <td>0.212600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421000</td>\n",
       "      <td>0.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421500</td>\n",
       "      <td>0.212800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422000</td>\n",
       "      <td>0.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422500</td>\n",
       "      <td>0.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423000</td>\n",
       "      <td>0.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423500</td>\n",
       "      <td>0.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424000</td>\n",
       "      <td>0.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424500</td>\n",
       "      <td>0.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425000</td>\n",
       "      <td>0.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425500</td>\n",
       "      <td>0.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426000</td>\n",
       "      <td>0.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426500</td>\n",
       "      <td>0.212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427000</td>\n",
       "      <td>0.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427500</td>\n",
       "      <td>0.212800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428000</td>\n",
       "      <td>0.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428500</td>\n",
       "      <td>0.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429000</td>\n",
       "      <td>0.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429500</td>\n",
       "      <td>0.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430000</td>\n",
       "      <td>0.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430500</td>\n",
       "      <td>0.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431000</td>\n",
       "      <td>0.211900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431500</td>\n",
       "      <td>0.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432000</td>\n",
       "      <td>0.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432500</td>\n",
       "      <td>0.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433000</td>\n",
       "      <td>0.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433500</td>\n",
       "      <td>0.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434000</td>\n",
       "      <td>0.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434500</td>\n",
       "      <td>0.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435000</td>\n",
       "      <td>0.210200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435500</td>\n",
       "      <td>0.210200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436000</td>\n",
       "      <td>0.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436500</td>\n",
       "      <td>0.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437000</td>\n",
       "      <td>0.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437500</td>\n",
       "      <td>0.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438000</td>\n",
       "      <td>0.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438500</td>\n",
       "      <td>0.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439000</td>\n",
       "      <td>0.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439500</td>\n",
       "      <td>0.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440000</td>\n",
       "      <td>0.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440500</td>\n",
       "      <td>0.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441000</td>\n",
       "      <td>0.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441500</td>\n",
       "      <td>0.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442000</td>\n",
       "      <td>0.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442500</td>\n",
       "      <td>0.210400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443000</td>\n",
       "      <td>0.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443500</td>\n",
       "      <td>0.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444000</td>\n",
       "      <td>0.208200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444500</td>\n",
       "      <td>0.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445000</td>\n",
       "      <td>0.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445500</td>\n",
       "      <td>0.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446000</td>\n",
       "      <td>0.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446500</td>\n",
       "      <td>0.210200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447000</td>\n",
       "      <td>0.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447500</td>\n",
       "      <td>0.211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448000</td>\n",
       "      <td>0.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448500</td>\n",
       "      <td>0.210300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449000</td>\n",
       "      <td>0.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449500</td>\n",
       "      <td>0.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450000</td>\n",
       "      <td>0.210200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450500</td>\n",
       "      <td>0.210200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451000</td>\n",
       "      <td>0.210600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451500</td>\n",
       "      <td>0.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452000</td>\n",
       "      <td>0.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452500</td>\n",
       "      <td>0.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>453000</td>\n",
       "      <td>0.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>453500</td>\n",
       "      <td>0.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>454000</td>\n",
       "      <td>0.209400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>454500</td>\n",
       "      <td>0.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455000</td>\n",
       "      <td>0.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455500</td>\n",
       "      <td>0.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456000</td>\n",
       "      <td>0.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456500</td>\n",
       "      <td>0.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457000</td>\n",
       "      <td>0.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457500</td>\n",
       "      <td>0.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458000</td>\n",
       "      <td>0.209400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458500</td>\n",
       "      <td>0.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459000</td>\n",
       "      <td>0.210200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459500</td>\n",
       "      <td>0.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460000</td>\n",
       "      <td>0.209900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460500</td>\n",
       "      <td>0.210100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14d 18h 10min 26s, sys: 20h 39min 45s, total: 15d 14h 50min 11s\n",
      "Wall time: 3d 11h 52min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=460776, training_loss=0.2530926650378937, metrics={'train_runtime': 301924.8492, 'train_samples_per_second': 586.031, 'train_steps_per_second': 1.526, 'total_flos': 2.3439905483365896e+19, 'train_loss': 0.2530926650378937, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()\n",
    "#trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab8b26d6-ec31-41f5-8f2c-de23acbed377",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./GrandBookData/PubchemModelDeberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cc9548e-a838-41f0-8af2-b584b835f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DebertaV2Tokenizer'. \n",
      "The class this function is called from is 'DebertaTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DebertaV2Tokenizer'. \n",
      "The class this function is called from is 'DebertaTokenizerFast'.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./GrandBookData/PubchemModelDeberta\",\n",
    "    tokenizer=DebertaTokenizerFast.from_pretrained(\"./GrandBookData/PubchemModelDeberta\", max_len=512),\n",
    "    framework = 'pt'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b36da2c9-ec7d-4c19-8958-aaa1b016663f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.8253141641616821,\n",
       "  'token': 39,\n",
       "  'token_str': 'C',\n",
       "  'sequence': 'CC(=O)OC(CC(=O)O)C[N+](C)(C)C'},\n",
       " {'score': 0.09695647656917572,\n",
       "  'token': 328,\n",
       "  'token_str': 'CCO',\n",
       "  'sequence': 'CC(=O)OC(CC(=O)O)C[N+](C)(C)CCO'},\n",
       " {'score': 0.013304267078638077,\n",
       "  'token': 350,\n",
       "  'token_str': 'CCCO',\n",
       "  'sequence': 'CC(=O)OC(CC(=O)O)C[N+](C)(C)CCCO'},\n",
       " {'score': 0.009745010174810886,\n",
       "  'token': 51,\n",
       "  'token_str': 'O',\n",
       "  'sequence': 'CC(=O)OC(CC(=O)O)C[N+](C)(C)O'},\n",
       " {'score': 0.007851950824260712,\n",
       "  'token': 283,\n",
       "  'token_str': 'CCN',\n",
       "  'sequence': 'CC(=O)OC(CC(=O)O)C[N+](C)(C)CCN'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"CC(=O)OC(CC(=O)O)C[N+](C)(C)[MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00173b40-1397-4a84-a75c-905fd53d71d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9832305908203125,\n",
       "  'token': 50,\n",
       "  'token_str': 'N',\n",
       "  'sequence': 'CC(=O)OC(CC(=O)O)C[N+](C)(C)C'},\n",
       " {'score': 0.015469036996364594,\n",
       "  'token': 52,\n",
       "  'token_str': 'P',\n",
       "  'sequence': 'CC(=O)OC(CC(=O)O)C[P+](C)(C)C'},\n",
       " {'score': 0.0003683821123559028,\n",
       "  'token': 725,\n",
       "  'token_str': 'As',\n",
       "  'sequence': 'CC(=O)OC(CC(=O)O)C[As+](C)(C)C'},\n",
       " {'score': 0.0001315820700256154,\n",
       "  'token': 55,\n",
       "  'token_str': 'S',\n",
       "  'sequence': 'CC(=O)OC(CC(=O)O)C[S+](C)(C)C'},\n",
       " {'score': 8.783095108810812e-05,\n",
       "  'token': 262,\n",
       "  'token_str': 'CC',\n",
       "  'sequence': 'CC(=O)OC(CC(=O)O)C[CC+](C)(C)C'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"CC(=O)OC(CC(=O)O)C[[MASK]+](C)(C)C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9af25f1-c3ae-437f-9eee-0225874def34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9215098023414612,\n",
       "  'token': 270,\n",
       "  'token_str': 'NC',\n",
       "  'sequence': 'CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2c-1nc1cc(F)c(C)c3c1c2[C@@H](NC(=O)COCNC(=O)[C@H](C)NC(=O)[C@@H](NC(=O)[C@@H](N)CCC(=O)NC[C@H]1O[C@@H](CC(N)=O)[C@H](O)[C@@H]1O)C(C)C)CC3'},\n",
       " {'score': 0.07309132814407349,\n",
       "  'token': 262,\n",
       "  'token_str': 'CC',\n",
       "  'sequence': 'CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2c-1nc1cc(F)c(C)c3c1c2[C@@H](NC(=O)COCNC(=O)[C@H](C)CC(=O)[C@@H](NC(=O)[C@@H](N)CCC(=O)NC[C@H]1O[C@@H](CC(N)=O)[C@H](O)[C@@H]1O)C(C)C)CC3'},\n",
       " {'score': 0.001541259465739131,\n",
       "  'token': 983,\n",
       "  'token_str': 'OCNC',\n",
       "  'sequence': 'CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2c-1nc1cc(F)c(C)c3c1c2[C@@H](NC(=O)COCNC(=O)[C@H](C)OCNC(=O)[C@@H](NC(=O)[C@@H](N)CCC(=O)NC[C@H]1O[C@@H](CC(N)=O)[C@H](O)[C@@H]1O)C(C)C)CC3'},\n",
       " {'score': 0.0013604763662442565,\n",
       "  'token': 304,\n",
       "  'token_str': 'CNC',\n",
       "  'sequence': 'CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2c-1nc1cc(F)c(C)c3c1c2[C@@H](NC(=O)COCNC(=O)[C@H](C)CNC(=O)[C@@H](NC(=O)[C@@H](N)CCC(=O)NC[C@H]1O[C@@H](CC(N)=O)[C@H](O)[C@@H]1O)C(C)C)CC3'},\n",
       " {'score': 0.0009771381737664342,\n",
       "  'token': 269,\n",
       "  'token_str': 'OC',\n",
       "  'sequence': 'CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2c-1nc1cc(F)c(C)c3c1c2[C@@H](NC(=O)COCNC(=O)[C@H](C)OC(=O)[C@@H](NC(=O)[C@@H](N)CCC(=O)NC[C@H]1O[C@@H](CC(N)=O)[C@H](O)[C@@H]1O)C(C)C)CC3'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2c-1nc1cc(F)c(C)c3c1c2[C@@H](NC(=O)COCNC(=O)[C@H](C)NC(=O)[C@@H](NC(=O)[C@@H](N)CCC(=O)NC[C@H]1O[C@@H](CC(N)=O)[C@H](O)[C@@H]1O)C(C)C)CC3\n",
    "fill_mask(\"CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2c-1nc1cc(F)c(C)c3c1c2[C@@H](NC(=O)COCNC(=O)[C@H](C)[MASK](=O)[C@@H](NC(=O)[C@@H](N)CCC(=O)NC[C@H]1O[C@@H](CC(N)=O)[C@H](O)[C@@H]1O)C(C)C)CC3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f36d8a76-600c-4c46-91df-a754857c8fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9355704188346863,\n",
       "  'token': 276,\n",
       "  'token_str': '@@',\n",
       "  'sequence': 'CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2c-1nc1cc(F)c(C)c3c1c2[C@@H](NC(=O)COCNC(=O)[C@H](C)NC(=O)[C@@H](NC(=O)[C@@H](N)CCC(=O)NC[C@H]1O[C@@H](CC(N)=O)[C@H](O)[C@@H]1O)C(C)C)CC3'},\n",
       " {'score': 0.06442631036043167,\n",
       "  'token': 36,\n",
       "  'token_str': '@',\n",
       "  'sequence': 'CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2c-1nc1cc(F)c(C)c3c1c2[C@@H](NC(=O)COCNC(=O)[C@H](C)NC(=O)[C@@H](NC(=O)[C@H](N)CCC(=O)NC[C@H]1O[C@@H](CC(N)=O)[C@H](O)[C@@H]1O)C(C)C)CC3'},\n",
       " {'score': 3.7355184190346336e-07,\n",
       "  'token': 324,\n",
       "  'token_str': '@]',\n",
       "  'sequence': 'CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2c-1nc1cc(F)c(C)c3c1c2[C@@H](NC(=O)COCNC(=O)[C@H](C)NC(=O)[C@@H](NC(=O)[C@]H](N)CCC(=O)NC[C@H]1O[C@@H](CC(N)=O)[C@H](O)[C@@H]1O)C(C)C)CC3'},\n",
       " {'score': 2.8162503440398723e-07,\n",
       "  'token': 263,\n",
       "  'token_str': '(=',\n",
       "  'sequence': 'CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2c-1nc1cc(F)c(C)c3c1c2[C@@H](NC(=O)COCNC(=O)[C@H](C)NC(=O)[C@@H](NC(=O)[C(=H](N)CCC(=O)NC[C@H]1O[C@@H](CC(N)=O)[C@H](O)[C@@H]1O)C(C)C)CC3'},\n",
       " {'score': 1.8820446712197736e-07,\n",
       "  'token': 400,\n",
       "  'token_str': '@](',\n",
       "  'sequence': 'CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2c-1nc1cc(F)c(C)c3c1c2[C@@H](NC(=O)COCNC(=O)[C@H](C)NC(=O)[C@@H](NC(=O)[C@](H](N)CCC(=O)NC[C@H]1O[C@@H](CC(N)=O)[C@H](O)[C@@H]1O)C(C)C)CC3'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2c-1nc1cc(F)c(C)c3c1c2[C@@H](NC(=O)COCNC(=O)[C@H](C)NC(=O)[C@@H](NC(=O)[C@@H](N)CCC(=O)NC[C@H]1O[C@@H](CC(N)=O)[C@H](O)[C@@H]1O)C(C)C)CC3\n",
    "fill_mask(\"CC[C@@]1(O)C(=O)OCc2c1cc1n(c2=O)Cc2c-1nc1cc(F)c(C)c3c1c2[C@@H](NC(=O)COCNC(=O)[C@H](C)NC(=O)[C@@H](NC(=O)[C[MASK]H](N)CCC(=O)NC[C@H]1O[C@@H](CC(N)=O)[C@H](O)[C@@H]1O)C(C)C)CC3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e589c5ca-58f1-4dfe-9a22-262725bcf857",
   "metadata": {},
   "source": [
    "# Running Protein pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9abb94cb-d01d-49e3-8254-18219432200f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m:  View run \u001b[33m./GrandBookData/OutProteinDeberta\u001b[0m at: \u001b[34mhttps://wandb.ai/vlad-mun/huggingface/runs/afgnmaw7\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250404_122058-afgnmaw7/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to load the vocab.json file\n",
    "def load_vocab_json(vocab_file):\n",
    "    with open(vocab_file+\".json\", 'r', encoding='utf-8') as f:\n",
    "        vocab = json.load(f)\n",
    "    return vocab\n",
    "\n",
    "# Function to load the merges.txt file\n",
    "def load_merges(merges_file):\n",
    "    with open(merges_file+\".txt\", 'r', encoding='utf-8') as f:\n",
    "        merges = [line.strip() for line in f.readlines()]\n",
    "    return merges\n",
    "\n",
    "# Function to identify n-grams\n",
    "def identify_ngrams(merges):\n",
    "    one_grams, two_grams, three_grams, four_grams = [], [], [], []\n",
    "    \n",
    "    for merge in merges:\n",
    "        tokens = merge.split()\n",
    "        if len(tokens) == 1:\n",
    "            one_grams.append(merge)\n",
    "        elif len(tokens) == 2:\n",
    "            two_grams.append(merge)\n",
    "        elif len(tokens) == 3:\n",
    "            three_grams.append(merge)\n",
    "        elif len(tokens) == 4:\n",
    "            four_grams.append(merge)\n",
    "    \n",
    "    return one_grams, two_grams, three_grams, four_grams\n",
    "\n",
    "# Function to filter out the 2-grams and 3-grams not in the 4-grams\n",
    "def filter_ngrams(two_grams, three_grams, four_grams):\n",
    "    # Create a set of 4-grams for fast lookup\n",
    "    four_gram_set = set(four_grams)\n",
    "    \n",
    "    # Filter 2-grams and 3-grams that exist in 4-grams\n",
    "    filtered_two_grams = [tg for tg in two_grams if tg in four_gram_set]\n",
    "    filtered_three_grams = [tg for tg in three_grams if tg in four_gram_set]\n",
    "    \n",
    "    return filtered_two_grams, filtered_three_grams\n",
    "\n",
    "# Function to update the merges.txt file\n",
    "def update_merges_file(merges_file, filtered_two_grams, filtered_three_grams, four_grams, one_grams):\n",
    "    # Combine the remaining n-grams\n",
    "    updated_merges = one_grams + filtered_two_grams + filtered_three_grams + four_grams\n",
    "    \n",
    "    with open(merges_file+\"_new.txt\", 'w', encoding='utf-8') as f:\n",
    "        for merge in updated_merges:\n",
    "            f.write(merge + '\\n')\n",
    "\n",
    "# Function to update vocab.json (removing 2-grams and 3-grams)\n",
    "def update_vocab_json(vocab_file, two_grams, three_grams):\n",
    "    vocab = load_vocab_json(vocab_file)\n",
    "    \n",
    "    # Assume vocab is a dictionary where keys are tokens and values are frequencies or other values\n",
    "    # If 2-grams and 3-grams are in the vocab, remove them\n",
    "    for tg in two_grams + three_grams:\n",
    "        vocab.pop(tg, None)  # Remove the token if it exists\n",
    "    \n",
    "    with open(vocab_file+\"new.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(vocab, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Main function to orchestrate the task\n",
    "def process_files(vocab_file, merges_file):\n",
    "    vocab = load_vocab_json(vocab_file)\n",
    "    merges = load_merges(merges_file)\n",
    "    \n",
    "    one_grams, two_grams, three_grams, four_grams = identify_ngrams(merges)\n",
    "    \n",
    "    filtered_two_grams, filtered_three_grams = filter_ngrams(two_grams, three_grams, four_grams)\n",
    "    \n",
    "    # Update merges.txt\n",
    "    update_merges_file(merges_file, filtered_two_grams, filtered_three_grams, four_grams, one_grams)\n",
    "    \n",
    "    # Update vocab.json if needed\n",
    "    update_vocab_json(vocab_file, filtered_two_grams, filtered_three_grams)\n",
    "\n",
    "# Example usage\n",
    "vocab_file = './GrandBookData/ProteinModelDeb/vocab'  # Path to your vocab.json\n",
    "merges_file = './GrandBookData/ProteinModelDeb/merges'  # Path to your merges.txt\n",
    "\n",
    "process_files(vocab_file, merges_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c76f131-cf69-428f-9760-8d979039c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import DebertaV2Config, DebertaV2Model\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "config = DebertaV2Config(\n",
    "    vocab_size=3262,\n",
    "    max_position_embeddings=512,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    intermediate_size=3072,\n",
    "    hidden_size = 768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "221b025f-54d6-47a3-a567-bfa6d6fa17e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaTokenizerFast(name_or_path='./GrandBookData/ProteinModelDeb/', vocab_size=3262, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DebertaTokenizerFast\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = DebertaTokenizerFast.from_pretrained(\"./GrandBookData/ProteinModelDeb/\", max_len=512)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa50bf0a-0624-4cbc-84d5-10840dd85059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DebertaV2ForMaskedLM\n",
    "\n",
    "model = DebertaV2ForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17f81dbe-e09b-4252-82a9-b5367cf6a3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46022590"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e9f8b94-7a2a-47e2-9fd0-388965bfccbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForMaskedLM(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(3262, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): LegacyDebertaV2OnlyMLMHead(\n",
       "    (predictions): LegacyDebertaV2LMPredictionHead(\n",
       "      (transform): LegacyDebertaV2PredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=3262, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e916341f-8557-40d7-9493-f9408cb31949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b564e906-3f31-480a-b0c5-808efa800b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'MAEIAFISRILMFCSVLVMLLCSVGLVVWGAFMTDSSYDRKLKDVVFNYNSSEPLAGDKSNMRSWYQCCGVRSGGWTCPNNTPCDTAVFNSVDNAMMIAGIIMIPLLLLQFFIIGFSALVLRIEPRVVKERKRGEEVTNTDETWSS'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files={'train': './GrandBookData/Proteins/train.json',\n",
    "                                              'test': './GrandBookData/Proteins/val.json'})\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "961ba6a5-11fe-4c82-804f-907d9ecd2139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1832414\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 879\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0ecfb35-f82f-454d-af2a-aa62e51a3742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1832414\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 879\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = dataset['train'].train_test_split(test_size=0.9)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1efde547-a19a-4f2c-af70-704505ee9221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=24): 100%|| 1832414/1832414 [00:22<00:00, 81556.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenization(example):\n",
    "    return tokenizer(example[\"text\"],padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "dataset = dataset['train'].map(tokenization, batched=True, num_proc=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d46f0b43-a562-4fc2-be38-0865af94980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch\n",
      "cuda:0\n",
      "DebertaV2ForMaskedLM(\n",
      "  (deberta): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(3262, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): LegacyDebertaV2OnlyMLMHead(\n",
      "    (predictions): LegacyDebertaV2LMPredictionHead(\n",
      "      (transform): LegacyDebertaV2PredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=3262, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.cuda\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"text\"])\n",
    "print(dataset.format['type'])\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f1d6298-0ab1-43b9-8583-08069da72ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'MILLFAIITEIIADFIYPDRANPIVPYRNNEDAYTFYDIFLSVSTRISSNGRLQITFPQEFDGTQLRLLQDDVMAATNIDKECYIEYTFSTTTTCIPYVEPAVTIPATNYYTISNRTITIFFGSEPLNAGSQKVRIKYNKNPVGLGGKSSGYLQIITLRDTVPVDQNLNFGTVGFAPVYKKHTIASEGQIINDGSKLGGYVTNYVIQFTTTIDLIKGSWFRAEIAEGFDISNVECVLVDTLQNLNCQVEGRRIYFSAIDAALPKGQHKVKLKNVINPSASGDSDPFIFETLEPLVNTVIEYFNVGVTNITPGQIINPSIAGAPLNQNLRIDYTIKFTPQNKIPYTGRIDYIIPDGFTLDPTCRIISGLSQNGTTPISCVTSGQKITITDFASFSPQEITLKIYAVNPSISKLYQYCQIYTYSTFGKVPVVSGTSSNYKGIDQNVEAGSIQISQIQSPYFVAIDLYKQMQNISLDKTGPLDFRLYPMPSKQLKLTTSPTSYGSIYFQIPLWWRMSGQYFNGWNVVGSTPTCNFGAEIAATCNYYLQRFSFKTATSTNYLKYDPNSGLGNCDLPISVAVSQITPIPGRFDFRVLTFNDGVYNTFDQTPLEQDLYTMEIAADQFIAPTTYIKCTSVDQVDADMVCYSNVLVMLPIPYEGAINFNMSTVNSTYESSAAWPNDLGFGLGNDGKKRIPCYVSIRASHPNIQCWVFSGGTTRKYAILQVRGFSSKVNTGTNVFVVIPNVKSCTAYDMQCFVTVSSAWTTELDEPYMMNYLKQSVGFIKQGDVYKDVTVSPPTLAKDVQDLTLSATFLSTRKQICELTTWTLQFQHTEVITACNTGTTNYPYCANQTPDHILIKFTDKTRFPNQFGVTATSPQGTVYLIDHFLTGEYYYFIIVNSGNVAANGPVQLSNIYNPIIYTPGIDIKLIYWNQNRKMKRITFPTSNIFIDGGILGPPQFDILPKTAVGMNADRSQVVDMDFDSWIIFRATFTTCHKIPANGLVELEVQNGLVDYTTTCEVWSGVTSPPKRLGHTHECNLVDVSGTKYYQLTKFNEIPVLTKFTLSFRGKTGLASTTANPIRIQTYFDATVPTSTASNVDNFYNTAYPPNPVLNGIDITGPKRFPPKLEWMDFIIINDKAREGTRGEFYMHVKTNHNIPDYTTNTAPRLVFTFNTTAVSMPQGAYPHCRLDGHLARFCDWAGGVITMAMPLTQGMIANQEHLVMISTRGADYSSTYQEGLHHNKAGTYKLKIETVNTGTNQFIRKVMTIDPPKFRVFWVWSANKVGYDPVTPSATLQGMSVFRLYFTNTITIPPSDNPDPISIIVLFHRTIWPTFTNGFEQDLGTGLPHRTQLPCIQDGFTLLPGATQLKCRLFYGSYPNPTKVIITDFDQINVNTNGEIHLPNIFNPNSTYELTKITLQVQKTDAITGTSELILSSDYDLINGTFGRTKEQPGFNKLAVDIPALVQDDLQNVWAAAVNPTFSVATIDTVATLSLQYGSKDYNMGGGADMILFEIPPNWDLPYGLITCSFPAGNICYSYPLGPYIGLYPDSGITADSLQSGSVTIKTPPFKVLVATSDPPIKIYVFTRSKLVYVYKIDFTQVLNAVAVTPAVTCTLCFYEAITDYTITVTNPKNIPTTGAITIKIPTGLTIRENGCRNDVASGSVLSKVGFECAYDGANSQLLITGFNKFIGPGKIIIKVRMENPDNTVTITEHWDFKTWYLNESPLNKLITYGDTATPALTAQTNIEYWDTPYRSRVRGRTTLYGSIEFRINLQNDLAAGDILFIKFPILFNKVRCSQYLCLWDDSPWFDNHNDNFLPAEKCLYDKSTRILSMYTPLSTKIKKDTLYRVIIDTQNSADGSRGFQLPAMNTYTFDIYTTLELKHSDIFVPQKQAFKYFECVSLITNVKSKNMMVVQIQSWTFGGGYLKLSFPTLSEESEFLYQDDLGTGKKDGDSIQCYARNGFPPSTVLDCKLYLGNRVYGQPAYILIQGWIGSLNVDPSFTTGYQELFSRVISPPFTGTDKYYEFDIDQFYTPDVIGDDKHIEMKLESYNGASNLLLDQSFSYDMTLKHYTITTGTLPKPTKSSDTLFDQSVTFDLKMQSAIQLKNTEEYDFFIFEYTQPYFDPQPQNLQCESGSVYVCRVSQGNNWIVVKPLSTIQAQANLKFKNSNNPIQVPTTGIVCTAYAVKDRIITTIYTYAAINDYLLPTLAATAYNTIVTPIDYADNTTIPKSTEIRVRVEITIPTQSITLPYGSIIDIFLPSGFTAYDYSLSTSDTNAKFVTADNLQTSVIASSTITGFTGRPQLTITGWEPIKVGEKIIFETYCKTSSSALSPALWNFKAWKDTGRTIELWTSAASAFNVQSYASFKTLDWKTKDPTVARKNQISPFTFHLSFKTTPTANTKITVSFPALALVSVPAGAVLYCTIEGATGLVHKFSYCYGTNSLDVIMLMPVSPVTFAASIQYKVTISSRGNSINHGLMFGASGVFTATLTQDNGDTGNLRFEILPTESFTYTQLRIVNSGIKTTGYTETALLFSFKNSVLIPKSGKIVIAFPNYAIDGTTKLFQENLQQSAVYNSGDLYPGCYSINSDLQNNGLNCRIFKNSGEFFQTLIVIQSFTSDISGKPLQIVIEQVTLPTSAASADVFLYTADVNNNKLEAYYFEDAVIPLVLDPSTMTTTITPTSATVQGTTLITFSGVNFGGTAFDGYSYVAYRFPVGYYDISSATVTTGGTCATTSVLEIYAHWIIQRFTGLTACTLNQNIVLANINQVYSAVTTYTNLAFQVYACLTRVCGSYSPASNPTLPYTAASAGSLYVADATAARYFPYSFYKFELVYTCGHNIPMGGQLILDTDPTRFQVQRIELKSGGITSVTYTNAAGSITTFTLLSSYLKAQTTIKFDVYGYLPSTIGASGVVTFTFSLKNRYNGYEISPDTLKPSWDVKTPVSIISSQNFASVTSSQSILSIILKPGVKNVDNDLHFQLPSISTMDTTNFKLYVFIEGMISGSGSCSISIAEGSSAGTPLCNVAGTDKLEITGIDPSTIQQEFYLLVQTVKITTYGLLPVTIQLIDNNGVVKNIFTGSVIVYEQQMESFGANFKHTTNGGGSNILEISFDPPAITNFNGFDGELFLDLLGSFVSLPTTIIKEQCDYASAQNSDCEFVPAFRLNSIGSATAHRDPFNNRIIISKFEQQSTSINSIKIWFQYQYSAPTYWIMTKMFQFGFYIKGQGNNQIYKFGESFIIKLSELNPSAPLSNSGSLNHIGSAQSIGTITSSTIYTQQSINSFADDDYFAIKLIGGYSVKYGTYNQMKMLSSTATISSSSTTTTLDNTYTNQRMGYFEGKMANGQSDIAFTLSNIELPYYPTSSLTDIAAIVLILDKTDRSIKEKIILTRSTSASSLSFATCSAQQIDSTSFEFSFTLPSNFKTFNHNGCVSTSTCYQYIEITTTNSAYNDIWSYEVTCGSGSSLINQYIPYSGYPPKFSQITSNPYGMRLSEYLEMNSNSQICIKYKKKTAAAAGSVQFTITIYENQSTPIAIASCTAASITASNTASGHQLSGVVKQPFGLQLFYDNFKAIVKGVERQLTIEFSLLADLSSSGTFTIESANLGLGNFDVTNRLKCMFKEKATQLAKHVPSLCTVDTTNKKITITYSVATVTTLNPKLVKDKVYELIIKYDQNLIKFLSEPALQGQYQLKFKFFAGATLTYSQVLPFTIYNNQVSDLVTQSYVDHISESTNFYVKYKTTQSLASSDYIDVLIPIRDVQSDGSLKILNTNNLNTGILNGQRIGCLDKVGNLGLYFKCYLYYGIVDFQKPHSFVRLTNFTGLGSGATVQFVIQNLKNPSTTDLIMDYEIRISGADSYEYTYAYAAAYTLPASSATSSTTGTLTITPLPCFSQINYQYQVSLTKDVDPNYESIVKISYDPLIWGEDYTIFAATIGGTASRIDPLDSQNVFIVFPTGYSTGTSITMAITNGLIQPSYVLTKSHAVTASFINYQTKTIVQVATTTTSVTTTQSNIANYGTVSSNSDLMKDFLSVMLIPMTLLGPVKTNYQFRIIFPTDYTLSGGYCKIFDDLTNLEYSTTCTDIDAQTKKIIFNTPIKATKAITAKIQVQTPTTPGSKSITVKFFADNLDTSTLLIQSNTLTAQTIINFTKMRLAMPYIKAPQILLQDSEIGPITLTVKLTSAITKQTDTVVIDTNNFMTVTIKQPINTKRLQFACFWNNLAAMNCRNGDTSANYDKITMYAPSDTGFASGDIVTIKIMVWRHQEIGQTDFTYQWHDDTDDSVPIANIKDKKITIGTTDYQKYVTYQNYRFDETTVPTYQTNVAVHQGTIDTQTDLTVSFRLRRTLSSNLNTNWGQIILKLPTMDDDNLQSQFPNGLGRFTKNAQEIDCFCSTCPQISNIDPLEPFFKCYVYQGSSHRVAPVEIWLNPQQDMVAQQIVEISFPKIKLPSTSYTYGKISMLAYEFQASTKSTRNHSEIVHFYTFHTIPDAFTTYTAITPPTFTVNMVGAMSGVTATVNNPYDDLLDYCHDRWVLSTDDVETIFEHPYNPETGTLYSNGGLYPYNYIEYTTATNHIVIIYKLAKWITFVPKVVLTKSATQTLTIQKFKNMPYKIPTGVNYKMTLFIEIGVRSYTEYSVFTTATQYRLAYQTSTTVKQMSIDPIPAPAETAAPQTYELKFLPYNKIPKNGKIVFTFPPLPSYDWVFTDQYCTVSQNLQDSGCDIIPASRIIIIKDWLNDYDPNIDGYLSVKFDIVNPTNVQTQVPFIWRTYWDQNQNTWLIDEETFFENTPLTQIIQGGNLIVTLWDRYQHPYIICGGRKGPIRLRFILRDQNLVYPNDYIELWMQNTFWPVSTQKYIVCYFNDPNDFQLVNIKSHRCDWTAGVNGGTLQIYIPEEIDIHVGDEWELVITTFGQFGGDGWVVRSPSDLNWITFFAMQNGVNTNDKGWFEAQIPGCPFDSFGFDCQSVGWEASTVGGDPTWTILNFTAQTETNAIPAGDPSLLTYSRLVFEFITHNELEESWPMDLTNRMAATDDRAQVACNGMYKTTGANRPLQPKGNTYIECYNIKGQDKVWNNTPSYIVGSYFDAIAAAERFHFQIADFRNGYVQGAFQHVKISVEVIQEDGTFYPQNEHYCWHLPPSLYAPPSPSAASTSTWGISRSQVALSQQQSYFGDNNVQWFDTIVYDFNYPYTLPMKGYCLQRRYNANFSDGYLSPCTPQHYNHWWVMSAAIQDPTIKWQQRESGNFINPPFVYKQNNPTNDPTISNNIAQASPLRIYIWRNNRLISNQTYQLNGGKISPTTGQLYIVSQTTTNDNAQNTMVRHHMGIHFQTTTSAMKHIRLWAPNDYQNIQDCKVNRGLVSRDPADLDNKITCVITKLPAPIDKWLIEIDNFQTYQGHGEDQWMIIDFNLTNPNSEKWTGSWYGKTYESSTNLSYVIDESKGTDGQTWVGGNTVKQPNLFRVWRNTISYENRRAQTDDYAEVHMRVLPRTSHPATTDNSNTQVQIWLPLAYDLANGGDALCQISNEYHSDLDSVNCKITSDRKITLSTDNTYGLKPECSMVTVTTKNAIGGNGIKLPSTPQDDTFQVFIDSTDSGGNVQREYNYNQAYAVPDPITSATDPTFSIKSTIREANQYTVIKATFTAPLDIPAGYDTSSSGQDPLSAPPIGTLRYKFNTKDFVNSGYPGFPLNLGQSSTDVLCQGFGNLKGTLQCTLIASSQQSPDYPAIIEVRGFDYISQGDLIEVHFLNIKNGQFYQNKGNVTLSAYKKKGDGTKVELIQDSTDFVPVYDPNCYGNLCYTVFTTCPTSAEILMDPNVVGAYMQVIINPISINCTTPATPYQPLQAGDQVVLTFPPEFEFPSESNGAISALWNQIQMSTIVYPLTREIYFTFPTGASVTGCTNTLKISQLRGPAYEFSTPYNIRMRIVKSQYRRVDCIISGITPPNVATPSTQSMLLSSYFAGDIFVDYDFKFSPSYKIPAGGSIDITFPNRGLLNYNHVTSSTPPAVCSLLNSVYITSCVLTTTGIKVTVAKDIPAQQELEIKLSGVKNSDYAGQTLASDYVLTMYHPNGEKVNEIGFSPFQFYAKKNVGVIYMKLSNTNNFQSVTASYTFVIQNSYRVPASGEIVLKMPKEWASVVTNSITLAKLGASWTSDALSYTYSIDSASDINNYLLRIKNQFTWPQGGSLTLLMLKLKNPSFESTKIFQAQTYYDNVLLDQTDPLDSSLKFTYKPAVPSLTINNFIMEPANAGEVSTYSLNMTSSGNMTSGSTIQIQFPTDTYPSGLTRSDLSLGCNLQFKNGTNVIVPCSAANSKLTVSLNANIDQNTEFTLSVVGITNPNFDTTAIKGSIDVLTTDSNNNVLTYNSGAAEINPTAAPSTMKLVKLATSSTNLQVKAAYTLCVQTDISIPLGAQVFVDFPQQFTFKSSSYPCYISLDHNNALLPYDNSTSSPSCKNSNSLRRIAVSGHTAAYVGNKNSPAQLCYILDNIENPSSAGPTDNFVVSIYDTQNKNIVAQTFGTLSPNSTLSYSQQGLVITVESIDPLPRYLTTKPIKVTLQRSVSHTVTLTPQSTEFKFSPPELVFLPSNGPEMYFKIQAINDDATEVGLKQFSWLKVESATAKFSEMADSFFQYVDKPTANQLYMTINPRVFRVALGGTSLPMTLTLSQPAAQDLLVNFTTINPYQDSSLVFEPINAAGQVKFPAGTTSIQLQYTTKQTALSGQIQFEIVDKYSSLYTILDNIVNFEILETDDKIPEVVNYYAVNVKRTSMYFRASIDESVTLYYYLTLKGNPKPTMEQIKSQQKLTNVFTQFGNNQSFIAPVTSDYIYNDVYLDLQGLTEQTDYSLYFFVTDLSGNNNTEVKQFDFTTATKYQPAQFKITLGKDVDIDKLLSAFGLVTGLPSSSFQTIEKPKKYTIDGELDSDVQSILDSQTVTYTFQINPDATVGGLSPYEYIRLIQQNLDLLQSEIPEVVDQNIFNTAWEYFEYPQEFKYNPIKINVTEDAVYFNVSLRYTGNLYTLVLPADSPAPSSKQVSMGLNSNNYPVQKEWVYKIRFEYSNKTSTDDQKLHAIFNYTLLFDNSYYKAYFTADNNLVSNPDLMTNEQMKQIEFKTKREIVIIPKKYLQSTLLSIIMIIILCIG', 'input_ids': tensor([   0,  420,  896,  950,  333,  360,  321,  341,  445,  425, 1308, 1766,\n",
      "         503,  312,  483,  349, 2302,  307,  292,  294,  287,  330,  263,  348,\n",
      "         269,  373, 1863, 2270,  295, 1215,  623,   53,  903,  983, 1500, 1480,\n",
      "         642, 2471,  371, 2536,  620,  439,   61, 2237,  273, 1239,  283, 2835,\n",
      "         323,  316, 1550, 1861,  356,  302, 1607,  278,  331,  334,  330,  424,\n",
      "        1612,  329,  270, 1241,  634, 1787,  360,  271,  358,  311,  329,  429,\n",
      "        1205,  356,  311,  496,  298,  571, 2188,  323,  276,  426,   53, 1839,\n",
      "         295,  567,  266,  372,  388,  372,  520,   42,  296,  323,  808,  361,\n",
      "         428,  458,  864,  284,  496,  307,  316,  362, 2182,  465, 1143,  291,\n",
      "         736,  354,  426, 1010,  518,  624,   40, 2780,  361,  492,  334,  795,\n",
      "        1310,  366,  530,  462, 1052,  341,  347,  716,  286,  523,  311,  351,\n",
      "         518,  344,  315, 1500,  324,  516,  360,  366,  297,  278,  577,  441,\n",
      "         291,  330,  404,  323,  522,  324,  418,  306,  569,  310,  330,  404,\n",
      "        1889, 1571,  271,  352,  620, 1177,  540,  331,  348,  296,  394,  400,\n",
      "         489, 1366,  306,  323, 1821, 1150,  300, 1230,  271, 1783,  273,  366,\n",
      "         297,  567,   61,  475,   39, 2420,  399,  287,  356, 1653,  281,  285,\n",
      "         618, 2931,  326,  429, 1244,  278,  297,  373,  331,  520, 1706,  339,\n",
      "         293, 1377,  408,  517,  337,  265,  374,  310,  286,  381,  269,  445,\n",
      "         437,  318,  289,  277,  296,  300,   56, 1419,  297,  518,  373, 2752,\n",
      "         551, 1594,  475,   42,  348,  608,  806,  287,  469,  587,  356,  864,\n",
      "         261,  620,  438, 1787,  375,  320,  419,  283, 1298,  313,  424, 1495,\n",
      "         540,  507, 2552,  394,  294,  273, 1187, 1249,  342,  375,  381,  787,\n",
      "        1989,  820,  515,  371,  429,  828,  506,  274,  527, 2131,  261,  429,\n",
      "         341,  298,  296, 1911,  566, 1988,  354, 1307,  407,  642,  316,  648,\n",
      "         340, 1719, 1838, 1407,  421, 1501,  523,  287,  368,  263, 1871,  533,\n",
      "         274,  496,  270,  507,  295,  282,  330,  943,  372,  297, 1079,  414,\n",
      "         337,  823,  512,   42,  509,  296, 1897,  557,  354,  338, 1023, 1829,\n",
      "         310, 1504,   42,  281,  439, 1651,  400, 1745,   40,  517,   39,  339,\n",
      "         311,  263,  411,  296,  272,  299,  569,   49,  498,  313,  408,  869,\n",
      "        1572,  378,  304,  495,  304,  311,  582,  271,  314, 1924,  274,  271,\n",
      "         574,  292,  287,  345,  373, 2600,  296,   59, 1143,  450,   44,  333,\n",
      "         401, 2709,   50,  310,  296,  438,  569,   39, 2000,  324,  584,  275,\n",
      "         457,   42,  363,  419, 1850, 2966,  315,  706,  300,  378,  311,  313,\n",
      "         459,   44,  292,  763, 3070,  341, 1937,  285,  344,  261,  348,  329,\n",
      "         289,  316,  559,  366, 2487,  324,  326,  307,  277,  559,  608,  418,\n",
      "         345,  453,  330,  371,  469,  969,  341,  844,  798, 1657,  412,  887,\n",
      "         419,  715,  498,  321, 1746,  281,  609, 2770,  428,  360,  458, 1454,\n",
      "         296,  699, 1565,  327,  689,  272,  417,  418,  689,  404,  296,  620,\n",
      "         417,   59,  743,  440, 1625,  783,  547,  405,   39,  882,  304,  285,\n",
      "         357,  455,  289, 2235,  343,  439,  268, 2235,  271,  320,  338,  419,\n",
      "         270,  276,  296,  327,  394, 2005,  399,  412,  283,  434,  695,  276,\n",
      "         344,  402,  432,  456,  349,  280,  366, 1033,  326,  307,  310,  485,\n",
      "         375,  280,  731,   59,  478,  341, 1839,    2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7119, 512, 512)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxLen = 0\n",
    "pos = 0\n",
    "while len(dataset[pos]['text'])<6000:\n",
    "    pos+=1\n",
    "target_check = dataset[pos]\n",
    "print(target_check)\n",
    "len(target_check['text']),len(target_check['input_ids']),len(target_check['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d0768a2-9511-4fd4-830c-3ce0f3333c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MILLFAIITEIIADFIYPDRANPIVPYRNNEDAYTFYDIFLSVSTRISSNGRLQITFPQEFDGTQLRLLQDDVMAATNIDKECYIEYTFSTTTTCIPYVEPAVTIPATNYYTISNRTITIFFGSEPLNAGSQKVRIKYNKNPVGLGGKSSGYLQIITLRDTVPVDQNLNFGTVGFAPVYKKHTIASEGQIINDGSKLGGYVTNYVIQFTTTIDLIKGSWFRAEIAEGFDISNVECVLVDTLQNLNCQVEGRRIYFSAIDAALPKGQHKVKLKNVINPSASGDSDPFIFETLEPLVNTVIEYFNVGVTNITPGQIINPSIAGAPLNQNLRIDYTIKFTPQNKIPYTGRIDYIIPDGFTLDPTCRIISGLSQNGTTPISCVTSGQKITITDFASFSPQEITLKIYAVNPSISKLYQYCQIYTYSTFGKVPVVSGTSSNYKGIDQNVEAGSIQISQIQSPYFVAIDLYKQMQNISLDKTGPLDFRLYPMPSKQLKLTTSPTSYGSIYFQIPLWWRMSGQYFNGWNVVGSTPTCNFGAEIAATCNYYLQRFSFKTATSTNYLKYDPNSGLGNCDLPISVAVSQITPIPGRFDFRVLTFNDGVYNTFDQTPLEQDLYTMEIAADQFIAPTTYIKCTSVDQVDADMVCYSNVLVMLPIPYEGAINFNMSTVNSTYESSAAWPNDLGFGLGNDGKKRIPCYVSIRASHPNIQCWVFSGGTTRKYAILQVRGFSSKVNTGTNVFVVIPNVKSCTAYDMQCFVTVSSAWTTELDEPYMMNYLKQSVGFIKQGDVYKDVTVSPPTLAKDVQDLTLSATFLSTRKQICELTTWTLQFQHTEVITACNTGTTNYPYCANQTPDHILIKFTDKTRFPNQFGVTATSPQGTVYLIDHFLTGEYYYFIIVNSGNVAANGPVQLSNIYNPIIYTPGIDIKLIYWNQNRKMKRITFPTSNIFIDGGILGPPQFDILPKTAVGMNADRSQVVDMDFDSWIIFRATFTTCHKIPANGLVELEVQNGLVDYTTTCEVWSGVTSPPKRLGHTHECNLVDVSGTKYYQLTKFNEIPVLTKFTLSFRGKTGLASTTANPIRIQTYFDATVPTSTASNVDNFYNTAYPPNPVLNGIDITGPKRFPPKLEWMDFIIINDKAREGTRGEFYMHVKTNHNIPDYTTNTAPRLVFTFNTTAVSMPQGAYPHCRLDGHLARFCDWAGGVITMAMPLTQGMIANQEHLVMISTRGADYSSTYQEGLHHNKAGTYKLKIETVNTGTNQFIRKVMTIDPPKFRVFWVWSANKVGYDPVTPSATLQGMSVFRLYFTNTITIPPSDNPDPISIIVLFHRTIWPTFTNGFEQDLGTGLPHRTQLPCIQDGFTLLPGATQLKCRLFYGSYPNPTKVIITDFDQINVNTNGEIHLPNIFNPNSTYELTKITLQVQKTDAITGTSELILSSDYDLINGTFGRTKEQPGFNKLAVDIPALVQDDLQNVWAAAVNPTFSVATIDTVATLSLQYGSKDYNMGGGADMILFEIPPNWDLPYGLITCSFPAGNICYSYPLGPYIGLYPDSGITADSLQSGSVTIKTPPFKVLVATSDPPIKIYVFTRSKLVYVYKIDFTQVLNAVAVTPAVTCTLCFYEAITDYTITVTNPKNIPTTGAITIKIPTGLTIRENGCRNDVASGSVLSKVGFECAYDGANSQLLITGFNKFIGPGKIIIKVRMENPDNTVTITEHWDFKTWYLNESPLNKLITYGDTATPALTAQTNIEYWDTPYRSRVRGRTTLYGSIEFRINLQNDLAAGDILFIKFPILFNKVRCSQYLCLWDDSPWFDNHNDNFLPAEKCLYDKSTRILSMYTPLSTKIKKDTLYRVIIDTQNSADGSRGFQLPAMNTYTFDIYTTLELKHSDIFVPQKQAFKYFECVSLITNVKSKNMMVVQIQSWTFGGGYLKLSFPTLSEESEFLYQDDLGTGKKDGDSIQCYARNGFPPSTVLDCKLYLGNRVYGQPAYILIQGWIGSLNVDPSFTTGYQELFSRVISPPFTGTDKYYEFDIDQFYTPDVIGDDKHIEMKLESYNGASNLLLDQSFSYDMTLKHYTITTGTLPKPTKSSDTLFDQSVTFDLKMQSAIQLKNTEEYDFFIFEYTQPYFDPQPQNLQCESGSVYVCRVSQGNNWIVVKPLSTIQAQANLKFKNSNNPIQVPTTGIVCTAYAVKDRIITTIYTYAAINDYLLPTLAATAYNTIVTPIDYADNTTIPKSTEIRVRVEITIPTQSITLPYGSIIDIFLPSGFTAYDYSLSTSDTNAKFVTADNLQTSVIASSTITGFTGRPQLTITGWEPIKVGEKIIFETYCKTSSSALSPALWNFKAWKDTGRTIELWTSAASAFNVQSYASFKTLDWKTKDPTVARKNQISPFTFHLSFKTTPTANTKITVSFPALALVSVPAGAVLYCTIEGATGLVHKFSYCYGTNSLDVIMLMPVSPVTFAASIQYKVTISSRGNSINHGLMFGASGVFTATLTQDNGDTGNLRFEILPTESFTYTQLRIVNSGIKTTGYTETALLFSFKNSVLIPKSGKIVIAFPNYAIDGTTKLFQENLQQSAVYNSGDLYPGCYSINSDLQNNGLNCRIFKNSGEFFQTLIVIQSFTSDISGKPLQIVIEQVTLPTSAASADVFLYTADVNNNKLEAYYFEDAVIPLVLDPSTMTTTITPTSATVQGTTLITFSGVNFGGTAFDGYSYVAYRFPVGYYDISSATVTTGGTCATTSVLEIYAHWIIQRFTGLTACTLNQNIVLANINQVYSAVTTYTNLAFQVYACLTRVCGSYSPASNPTLPYTAASAGSLYVADATAARYFPYSFYKFELVYTCGHNIPMGGQLILDTDPTRFQVQRIELKSGGITSVTYTNAAGSITTFTLLSSYLKAQTTIKFDVYGYLPSTIGASGVVTFTFSLKNRYNGYEISPDTLKPSWDVKTPVSIISSQNFASVTSSQSILSIILKPGVKNVDNDLHFQLPSISTMDTTNFKLYVFIEGMISGSGSCSISIAEGSSAGTPLCNVAGTDKLEITGIDPSTIQQEFYLLVQTVKITTYGLLPVTIQLIDNNGVVKNIFTGSVIVYEQQMESFGANFKHTTNGGGSNILEISFDPPAITNFNGFDGELFLDLLGSFVSLPTTIIKEQCDYASAQNSDCEFVPAFRLNSIGSATAHRDPFNNRIIISKFEQQSTSINSIKIWFQYQYSAPTYWIMTKMFQFGFYIKGQGNNQIYKFGESFIIKLSELNPSAPLSNSGSLNHIGSAQSIGTITSSTIYTQQSINSFADDDYFAIKLIGGYSVKYGTYNQMKMLSSTATISSSSTTTTLDNTYTNQRMGYFEGKMANGQSDIAFTLSNIELPYYPTSSLTDIAAIVLILDKTDRSIKEKIILTRSTSASSLSFATCSAQQIDSTSFEFSFTLPSNFKTFNHNGCVSTSTCYQYIEITTTNSAYNDIWSYEVTCGSGSSLINQYIPYSGYPPKFSQITSNPYGMRLSEYLEMNSNSQICIKYKKKTAAAAGSVQFTITIYENQSTPIAIASCTAASITASNTASGHQLSGVVKQPFGLQLFYDNFKAIVKGVERQLTIEFSLLADLSSSGTFTIESANLGLGNFDVTNRLKCMFKEKATQLAKHVPSLCTVDTTNKKITITYSVATVTTLNPKLVKDKVYELIIKYDQNLIKFLSEPALQGQYQLKFKFFAGATLTYSQVLPFTIYNNQVSDLVTQSYVDHISESTNFYVKYKTTQSLASSDYIDVLIPIRDVQSDGSLKILNTNNLNTGILNGQRIGCLDKVGNLGLYFKCYLYYGIVDFQKPHSFVRLTNFTGLGSGATVQFVIQNLKNPSTTDLIMDYEIRISGADSYEYTYAYAAAYTLPASSATSSTTGTLTITPLPCFSQINYQYQVSLTKDVDPNYESIVKISYDPLIWGEDYTIFAATIGGTASRIDPLDSQNVFIVFPTGYSTGTSITMAITNGLIQPSYVLTKSHAVTASFINYQTKTIVQVATTTTSVTTTQSNIANYGTVSSNSDLMKDFLSVMLIPMTLLGPVKTNYQFRIIFPTDYTLSGGYCKIFDDLTNLEYSTTCTDIDAQTKKIIFNTPIKATKAITAKIQVQTPTTPGSKSITVKFFADNLDTSTLLIQSNTLTAQTIINFTKMRLAMPYIKAPQILLQDSEIGPITLTVKLTSAITKQTDTVVIDTNNFMTVTIKQPINTKRLQFACFWNNLAAMNCRNGDTSANYDKITMYAPSDTGFASGDIVTIKIMVWRHQEIGQTDFTYQWHDDTDDSVPIANIKDKKITIGTTDYQKYVTYQNYRFDETTVPTYQTNVAVHQGTIDTQTDLTVSFRLRRTLSSNLNTNWGQIILKLPTMDDDNLQSQFPNGLGRFTKNAQEIDCFCSTCPQISNIDPLEPFFKCYVYQGSSHRVAPVEIWLNPQQDMVAQQIVEISFPKIKLPSTSYTYGKISMLAYEFQASTKSTRNHSEIVHFYTFHTIPDAFTTYTAITPPTFTVNMVGAMSGVTATVNNPYDDLLDYCHDRWVLSTDDVETIFEHPYNPETGTLYSNGGLYPYNYIEYTTATNHIVIIYKLAKWITFVPKVVLTKSATQTLTIQKFKNMPYKIPTGVNYKMTLFIEIGVRSYTEYSVFTTATQYRLAYQTSTTVKQMSIDPIPAPAETAAPQTYELKFLPYNKIPKNGKIVFTFPPLPSYDWVFTDQYCTVSQNLQDSGCDIIPASRIIIIKDWLNDYDPNIDGYLSVKFDIVNPTNVQTQVPFIWRTYWDQNQNTWLIDEETFFENTPLTQIIQGGNLIVTLWDRYQHPYIICGGRKGPIRLRFILRDQNLVYPNDYIELWMQNTFWPVSTQKYIVCYFNDPNDFQLVNIKSHRCDWTAGVNGGTLQIYIPEEIDIHVGDEWELVITTFGQFGGDGWVVRSPSDLNWITFFAMQNGVNTNDKGWFEAQIPGCPFDSFGFDCQSVGWEASTVGGDPTWTILNFTAQTETNAIPAGDPSLLTYSRLVFEFITHNELEESWPMDLTNRMAATDDRAQVACNGMYKTTGANRPLQPKGNTYIECYNIKGQDKVWNNTPSYIVGSYFDAIAAAERFHFQIADFRNGYVQGAFQHVKISVEVIQEDGTFYPQNEHYCWHLPPSLYAPPSPSAASTSTWGISRSQVALSQQQSYFGDNNVQWFDTIVYDFNYPYTLPMKGYCLQRRYNANFSDGYLSPCTPQHYNHWWVMSAAIQDPTIKWQQRESGNFINPPFVYKQNNPTNDPTISNNIAQASPLRIYIWRNNRLISNQTYQLNGGKISPTTGQLYIVSQTTTNDNAQNTMVRHHMGIHFQTTTSAMKHIRLWAPNDYQNIQDCKVNRGLVSRDPADLDNKITCVITKLPAPIDKWLIEIDNFQTYQGHGEDQWMIIDFNLTNPNSEKWTGSWYGKTYESSTNLSYVIDESKGTDGQTWVGGNTVKQPNLFRVWRNTISYENRRAQTDDYAEVHMRVLPRTSHPATTDNSNTQVQIWLPLAYDLANGGDALCQISNEYHSDLDSVNCKITSDRKITLSTDNTYGLKPECSMVTVTTKNAIGGNGIKLPSTPQDDTFQVFIDSTDSGGNVQREYNYNQAYAVPDPITSATDPTFSIKSTIREANQYTVIKATFTAPLDIPAGYDTSSSGQDPLSAPPIGTLRYKFNTKDFVNSGYPGFPLNLGQSSTDVLCQGFGNLKGTLQCTLIASSQQSPDYPAIIEVRGFDYISQGDLIEVHFLNIKNGQFYQNKGNVTLSAYKKKGDGTKVELIQDSTDFVPVYDPNCYGNLCYTVFTTCPTSAEILMDPNVVGAYMQVIINPISINCTTPATPYQPLQAGDQVVLTFPPEFEFPSESNGAISALWNQIQMSTIVYPLTREIYFTFPTGASVTGCTNTLKISQLRGPAYEFSTPYNIRMRIVKSQYRRVDCIISGITPPNVATPSTQSMLLSSYFAGDIFVDYDFKFSPSYKIPAGGSIDITFPNRGLLNYNHVTSSTPPAVCSLLNSVYITSCVLTTTGIKVTVAKDIPAQQELEIKLSGVKNSDYAGQTLASDYVLTMYHPNGEKVNEIGFSPFQFYAKKNVGVIYMKLSNTNNFQSVTASYTFVIQNSYRVPASGEIVLKMPKEWASVVTNSITLAKLGASWTSDALSYTYSIDSASDINNYLLRIKNQFTWPQGGSLTLLMLKLKNPSFESTKIFQAQTYYDNVLLDQTDPLDSSLKFTYKPAVPSLTINNFIMEPANAGEVSTYSLNMTSSGNMTSGSTIQIQFPTDTYPSGLTRSDLSLGCNLQFKNGTNVIVPCSAANSKLTVSLNANIDQNTEFTLSVVGITNPNFDTTAIKGSIDVLTTDSNNNVLTYNSGAAEINPTAAPSTMKLVKLATSSTNLQVKAAYTLCVQTDISIPLGAQVFVDFPQQFTFKSSSYPCYISLDHNNALLPYDNSTSSPSCKNSNSLRRIAVSGHTAAYVGNKNSPAQLCYILDNIENPSSAGPTDNFVVSIYDTQNKNIVAQTFGTLSPNSTLSYSQQGLVITVESIDPLPRYLTTKPIKVTLQRSVSHTVTLTPQSTEFKFSPPELVFLPSNGPEMYFKIQAINDDATEVGLKQFSWLKVESATAKFSEMADSFFQYVDKPTANQLYMTINPRVFRVALGGTSLPMTLTLSQPAAQDLLVNFTTINPYQDSSLVFEPINAAGQVKFPAGTTSIQLQYTTKQTALSGQIQFEIVDKYSSLYTILDNIVNFEILETDDKIPEVVNYYAVNVKRTSMYFRASIDESVTLYYYLTLKGNPKPTMEQIKSQQKLTNVFTQFGNNQSFIAPVTSDYIYNDVYLDLQGLTEQTDYSLYFFVTDLSGNNNTEVKQFDFTTATKYQPAQFKITLGKDVDIDKLLSAFGLVTGLPSSSFQTIEKPKKYTIDGELDSDVQSILDSQTVTYTFQINPDATVGGLSPYEYIRLIQQNLDLLQSEIPEVVDQNIFNTAWEYFEYPQEFKYNPIKINVTEDAVYFNVSLRYTGNLYTLVLPADSPAPSSKQVSMGLNSNNYPVQKEWVYKIRFEYSNKTSTDDQKLHAIFNYTLLFDNSYYKAYFTADNNLVSNPDLMTNEQMKQIEFKTKREIVIIPKKYLQSTLLSIIMIIILCIG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS]MILLFAIITEIIADFIYPDRANPIVPYRNNEDAYTFYDIFLSVSTRISSNGRLQITFPQEFDGTQLRLLQDDVMAATNIDKECYIEYTFSTTTTCIPYVEPAVTIPATNYYTISNRTITIFFGSEPLNAGSQKVRIKYNKNPVGLGGKSSGYLQIITLRDTVPVDQNLNFGTVGFAPVYKKHTIASEGQIINDGSKLGGYVTNYVIQFTTTIDLIKGSWFRAEIAEGFDISNVECVLVDTLQNLNCQVEGRRIYFSAIDAALPKGQHKVKLKNVINPSASGDSDPFIFETLEPLVNTVIEYFNVGVTNITPGQIINPSIAGAPLNQNLRIDYTIKFTPQNKIPYTGRIDYIIPDGFTLDPTCRIISGLSQNGTTPISCVTSGQKITITDFASFSPQEITLKIYAVNPSISKLYQYCQIYTYSTFGKVPVVSGTSSNYKGIDQNVEAGSIQISQIQSPYFVAIDLYKQMQNISLDKTGPLDFRLYPMPSKQLKLTTSPTSYGSIYFQIPLWWRMSGQYFNGWNVVGSTPTCNFGAEIAATCNYYLQRFSFKTATSTNYLKYDPNSGLGNCDLPISVAVSQITPIPGRFDFRVLTFNDGVYNTFDQTPLEQDLYTMEIAADQFIAPTTYIKCTSVDQVDADMVCYSNVLVMLPIPYEGAINFNMSTVNSTYESSAAWPNDLGFGLGNDGKKRIPCYVSIRASHPNIQCWVFSGGTTRKYAILQVRGFSSKVNTGTNVFVVIPNVKSCTAYDMQCFVTVSSAWTTELDEPYMMNYLKQSVGFIKQGDVYKDVTVSPPTLAKDVQDLTLSATFLSTRKQICELTTWTLQFQHTEVITACNTGTTNYPYCANQTPDHILIKFTDKTRFPNQFGVTATSPQGTVYLIDHFLTGEYYYFIIVNSGNVAANGPVQLSNIYNPIIYTPGIDIKLIYWNQNRKMKRITFPTSNIFIDGGILGPPQFDILPKTAVGMNADRSQVVDMDFDSWIIFRATFTTCHKIPANGLVELEVQNGLVDYTTTCEVWSGVTSPPKRLGHTHECNLVDVSGTKYYQLTKFNEIPVLTKFTLSFRGKTGLASTTANPIRIQTYFDATVPTSTASNVDNFYNTAYPPNPVLNGIDITGPKRFPPKLEWMDFIIIN[SEP]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(target_check['text'])\n",
    "tokenizer.decode(target_check['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e65ca536-ec1c-4c90-b3ed-932312eb7816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForMaskedLM(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(3262, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): LegacyDebertaV2OnlyMLMHead(\n",
       "    (predictions): LegacyDebertaV2LMPredictionHead(\n",
       "      (transform): LegacyDebertaV2PredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=3262, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5c0f84f-b4d3-463c-94ab-25f925e72f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variables in the notebook\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46fc911d-f8c8-4636-8d97-c927a49a0461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam8bit (\n",
       "Parameter Group 0\n",
       "    alpha: 0.0\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0001\n",
       "    t_alpha: None\n",
       "    t_beta3: None\n",
       "    weight_decay: 0.001\n",
       "\n",
       "Parameter Group 1\n",
       "    alpha: 0.0\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0001\n",
       "    t_alpha: None\n",
       "    t_beta3: None\n",
       "    weight_decay: 0.0\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import bitsandbytes as bnb\n",
    "from torch import nn\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "def get_optimizer(model,n):\n",
    "    training_args = TrainingArguments(\n",
    "        #torch_compile=True,\n",
    "        output_dir=\"./GrandBookData/OutProteinDeberta\",\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay = 1e-3,\n",
    "        do_train=True,\n",
    "        num_train_epochs=20,\n",
    "        fp16=True,\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=2,\n",
    "        #NCCL_P2P_DISABLE=\"1\", \n",
    "        #NCCL_IB_DISABLE=\"1\",\n",
    "        #gradient_checkpointing=True,\n",
    "        save_steps=5_000,\n",
    "    save_total_limit=5,\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "    #model = multitask_model.taskmodels_dict[\"tox21\"]\n",
    "    decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "            \"weight_decay\": training_args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    optimizer_kwargs = {\n",
    "        \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n",
    "        \"eps\": training_args.adam_epsilon,\n",
    "    }\n",
    "    optimizer_kwargs[\"lr\"] = training_args.learning_rate\n",
    "    adam_bnb_optim = bnb.optim.Adam8bit(\n",
    "        optimizer_grouped_parameters,\n",
    "        betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
    "        eps=training_args.adam_epsilon,\n",
    "        lr=training_args.learning_rate,\n",
    "    )\n",
    "    return adam_bnb_optim, training_args\n",
    "\n",
    "optimizer,training_args = get_optimizer(model,\"e\")\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0acb39fa-31ff-4f68-bf6f-4d82d8218be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    optimizers=(optimizer, None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0b422a1-a9c9-4aa9-9a4a-219200d68d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvlad-mun\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vlad/FebReserach/Work ML/Paracelsus_Main/wandb/run-20250404_130409-zdqvnjvc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vlad-mun/huggingface/runs/zdqvnjvc' target=\"_blank\">./GrandBookData/OutProteinDeberta</a></strong> to <a href='https://wandb.ai/vlad-mun/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vlad-mun/huggingface' target=\"_blank\">https://wandb.ai/vlad-mun/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vlad-mun/huggingface/runs/zdqvnjvc' target=\"_blank\">https://wandb.ai/vlad-mun/huggingface/runs/zdqvnjvc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95440' max='95440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [95440/95440 16:56:10, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.339300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>6.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>6.155400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>6.150900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>6.143200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>6.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>6.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>6.119100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>6.110900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>6.106300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>6.085100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>6.040700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>5.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>5.892100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>5.856200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>5.828800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>5.808800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>5.789400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>5.771500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>5.756300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>5.747200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>5.736700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>5.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>5.719200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>5.712700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>5.703600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>5.696800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>5.688900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>5.686200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>5.678300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>5.673800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>5.667400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>5.658400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>5.658200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>5.655200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>5.650400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>5.642200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>5.643200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>5.637100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>5.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>5.631900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>5.627600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>5.623600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>5.622300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>5.620200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>5.618200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>5.610800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>5.608600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>5.607800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>5.606300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>5.606200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>5.604100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>5.597600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>5.600100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>5.594900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>5.593000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>5.591900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>5.591500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>5.586900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>5.587400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>5.580400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>5.586200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>5.580700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>5.582700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>5.578200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>5.576600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>5.573100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>5.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>5.567800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>5.568700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>5.570400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>5.568600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>5.564700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>5.562100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>5.565800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>5.562100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>5.558600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>5.557600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>5.553900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>5.554300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>5.556200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>5.552500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>5.553000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>5.549500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>5.547900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>5.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>5.547000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>5.547200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>5.545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>5.545200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>5.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>5.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>5.540400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>5.539300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>5.539200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>5.536400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>5.537100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>5.532900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>5.532500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>5.534200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>5.531600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>5.529800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>5.533100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>5.528100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>5.527600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>5.529700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>5.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>5.525300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>5.524400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>5.523900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>5.522400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>5.522600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>5.523100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>5.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>5.519700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>5.521400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>5.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>5.518200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>5.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>5.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>5.513900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>5.513600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>5.517700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>5.514800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>5.512400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>5.513200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>5.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>5.510500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>5.510900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>5.506400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>5.509400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>5.504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>5.509700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>5.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>5.507600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>5.504400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>5.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>5.503500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>5.502100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>5.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>5.503100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>5.503900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>5.496100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>5.499800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>5.498900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>5.499600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>5.496600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>5.498500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>5.503400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>5.496600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>5.497400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>5.496300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>5.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>5.493400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>5.496400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>5.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>5.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79500</td>\n",
       "      <td>5.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>5.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80500</td>\n",
       "      <td>5.489300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>5.496300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81500</td>\n",
       "      <td>5.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>5.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>5.488900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>5.491500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83500</td>\n",
       "      <td>5.488700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>5.491800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84500</td>\n",
       "      <td>5.489400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>5.486200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85500</td>\n",
       "      <td>5.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>5.488200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86500</td>\n",
       "      <td>5.486300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>5.490800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87500</td>\n",
       "      <td>5.486200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>5.487000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88500</td>\n",
       "      <td>5.487300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>5.488100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89500</td>\n",
       "      <td>5.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>5.486400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90500</td>\n",
       "      <td>5.487000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>5.485200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91500</td>\n",
       "      <td>5.485600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>5.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92500</td>\n",
       "      <td>5.484500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>5.484200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93500</td>\n",
       "      <td>5.488800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>5.483400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94500</td>\n",
       "      <td>5.483000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>5.485200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/vlad/anaconda3/envs/Baza/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3d 4min 13s, sys: 4h 25min 57s, total: 3d 4h 30min 11s\n",
      "Wall time: 16h 56min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=95440, training_loss=5.6007554750530435, metrics={'train_runtime': 60974.1588, 'train_samples_per_second': 601.046, 'train_steps_per_second': 1.565, 'total_flos': 4.855069339079393e+18, 'train_loss': 5.6007554750530435, 'epoch': 20.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()\n",
    "#trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0783dd76-8d6d-4664-91c2-016c77b31416",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./GrandBookData/ProteinModelDeb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56d689fb-a8ce-4cda-9eae-6522eea7b7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DebertaV2Tokenizer'. \n",
      "The class this function is called from is 'DebertaTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DebertaV2Tokenizer'. \n",
      "The class this function is called from is 'DebertaTokenizerFast'.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./GrandBookData/ProteinModelDeb\",\n",
    "    tokenizer=DebertaTokenizerFast.from_pretrained(\"./GrandBookData/ProteinModelDeb\", max_len=512),\n",
    "    framework = 'pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1619cf2-de2e-46ad-b05c-da4609ab09e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1583409607410431,\n",
       "  'token': 473,\n",
       "  'token_str': 'ASS',\n",
       "  'sequence': 'RQTYTRYQTLELEKEFHFNRYLTRRRRIEIAHALCLTERQIKIWFQNRRMKWKKENKTKGDIGANDGSDLSPQTSASSQ.'},\n",
       " {'score': 0.06732732057571411,\n",
       "  'token': 629,\n",
       "  'token_str': 'PPL',\n",
       "  'sequence': 'RQTYTRYQTLELEKEFHFNRYLTRRRRIEIAHALCLTERQIKIWFQNRRMKWKKENKTKGDIGANDGSDLSPQTSPPLQ.'},\n",
       " {'score': 0.057589925825595856,\n",
       "  'token': 486,\n",
       "  'token_str': 'ASL',\n",
       "  'sequence': 'RQTYTRYQTLELEKEFHFNRYLTRRRRIEIAHALCLTERQIKIWFQNRRMKWKKENKTKGDIGANDGSDLSPQTSASLQ.'},\n",
       " {'score': 0.056378092616796494,\n",
       "  'token': 396,\n",
       "  'token_str': 'AC',\n",
       "  'sequence': 'RQTYTRYQTLELEKEFHFNRYLTRRRRIEIAHALCLTERQIKIWFQNRRMKWKKENKTKGDIGANDGSDLSPQTSACQ.'},\n",
       " {'score': 0.038312628865242004,\n",
       "  'token': 548,\n",
       "  'token_str': 'APP',\n",
       "  'sequence': 'RQTYTRYQTLELEKEFHFNRYLTRRRRIEIAHALCLTERQIKIWFQNRRMKWKKENKTKGDIGANDGSDLSPQTSAPPQ.'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RQTYTRYQTLELEKEFHFNRYLTRRRRIEIAHALCLTERQIKIWFQNRRMKWKKENKTKGDIGANDGSDLSPQTSPQ\n",
    "fill_mask(\"RQTYTRYQTLELEKEFHFNRYLTRRRRIEIAHALCLTERQIKIWFQNRRMKWKKENKTKGDIGANDGSDLSPQTS[MASK]Q.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bb201c3-2500-47f5-bcd7-49def1a307aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.39751461148262024,\n",
       "  'token': 1443,\n",
       "  'token_str': 'RYL',\n",
       "  'sequence': 'RQTYTRYQTLELEKEFHFNRYLTRRRRIEIAHALCLTERQIKIWFQNRRMKWKKENKTKGDIGANDGSDLSPQTSPQ'},\n",
       " {'score': 0.11309065669775009,\n",
       "  'token': 313,\n",
       "  'token_str': 'YL',\n",
       "  'sequence': 'RQTYTRYQTLELEKEFHFNYLTRRRRIEIAHALCLTERQIKIWFQNRRMKWKKENKTKGDIGANDGSDLSPQTSPQ'},\n",
       " {'score': 0.05871174484491348,\n",
       "  'token': 2711,\n",
       "  'token_str': 'MYL',\n",
       "  'sequence': 'RQTYTRYQTLELEKEFHFNMYLTRRRRIEIAHALCLTERQIKIWFQNRRMKWKKENKTKGDIGANDGSDLSPQTSPQ'},\n",
       " {'score': 0.030161689966917038,\n",
       "  'token': 262,\n",
       "  'token_str': 'LL',\n",
       "  'sequence': 'RQTYTRYQTLELEKEFHFNLLTRRRRIEIAHALCLTERQIKIWFQNRRMKWKKENKTKGDIGANDGSDLSPQTSPQ'},\n",
       " {'score': 0.022992780432105064,\n",
       "  'token': 292,\n",
       "  'token_str': 'FL',\n",
       "  'sequence': 'RQTYTRYQTLELEKEFHFNFLTRRRRIEIAHALCLTERQIKIWFQNRRMKWKKENKTKGDIGANDGSDLSPQTSPQ'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RQTYTRYQTLELEKEFHFNRYLTRRRRIEIAHALCLTERQIKIWFQNRRMKWKKENKTKGDIGANDGSDLSPQTSPQ\n",
    "fill_mask(\"RQTYTRYQTLELEKEFHFN[MASK]TRRRRIEIAHALCLTERQIKIWFQNRRMKWKKENKTKGDIGANDGSDLSPQTSPQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b339da90-4d74-47a2-b4c8-a5a1d6a32279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
